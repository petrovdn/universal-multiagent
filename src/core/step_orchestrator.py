"""
Step Orchestrator for multi-step execution with planning and confirmation.
Implements plan_and_confirm and plan_and_execute modes with streaming.

DEPRECATED: This module is deprecated in favor of UnifiedReActEngine with PlanModeAdapter.
Use PlanModeAdapter instead for plan-based execution.
This module is kept for backward compatibility and will be removed in a future version.
"""

from typing import Dict, Any, List, Optional, AsyncGenerator
import asyncio
import json
import re
import time
from uuid import uuid4

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.language_models.chat_models import BaseChatModel

from src.core.context_manager import ConversationContext
from src.core.file_context_resolver import FileContextResolver
from src.api.websocket_manager import WebSocketManager
from src.agents.model_factory import create_llm
from src.utils.logging_config import get_logger
from src.utils.capabilities import get_available_capabilities, build_step_executor_prompt, build_planning_prompt
from src.mcp_tools.workspace_tools import get_workspace_tools
from src.mcp_tools.sheets_tools import get_sheets_tools
from src.mcp_tools.gmail_tools import get_gmail_tools
from src.mcp_tools.calendar_tools import get_calendar_tools
from src.mcp_tools.slides_tools import get_slides_tools
from src.mcp_tools.docs_tools import get_docs_tools

logger = get_logger(__name__)


def _escape_braces_for_fstring(text: str) -> str:
    """
    Escape curly braces in text to safely use in f-strings.
    Doubles all { and } characters so they are treated as literals.
    
    Args:
        text: Text that may contain curly braces
        
    Returns:
        Text with escaped braces
    """
    return text.replace("{", "{{").replace("}", "}}")


class StepOrchestrator:
    """
    Orchestrator for multi-step execution with planning and optional confirmation.
    
    Modes:
    - plan_and_confirm: Generate plan, wait for approval, then execute steps
    - plan_and_execute: Generate plan and execute immediately
    """
    
    def __init__(
        self,
        ws_manager: WebSocketManager,
        session_id: str,
        model_name: Optional[str] = None
    ):
        """
        Initialize StepOrchestrator.
        
        Args:
            ws_manager: WebSocket manager for sending events
            session_id: Session identifier
            model_name: Model name for LLM (optional, uses default from config if None)
        """
        self.ws_manager = ws_manager
        self.session_id = session_id
        self.model_name = model_name
        
        # Load tools for step execution
        self.tools = self._load_tools()
        
        # Create LLM with extended thinking support
        self.llm = self._create_llm_with_thinking()
        
        # Bind tools to LLM
        self.llm_with_tools = self.llm.bind_tools(self.tools)
        
        # State for plan confirmation
        self._confirmation_event: Optional[asyncio.Event] = None
        self._confirmation_result: Optional[bool] = None
        self._plan_steps: List[str] = []
        self._plan_text: str = ""
        self._confirmation_id: Optional[str] = None
        
        # State for user assistance requests
        self._user_assistance_event: Optional[asyncio.Event] = None
        self._user_assistance_result: Optional[Dict[str, Any]] = None
        self._user_assistance_id: Optional[str] = None
        self._user_assistance_context: Optional[Dict[str, Any]] = None
        self._user_assistance_options: Optional[List[Dict[str, Any]]] = None
        
        self._stop_requested: bool = False
        self._streaming_task: Optional[asyncio.Task] = None
        # Track sent workspace events to prevent duplicates (key: (session_id, tool_name, spreadsheet_id), value: timestamp)
        self._sent_workspace_events: Dict[str, float] = {}
        self._current_step_index: Optional[int] = None  # Track current step index for file_preview events
        
        logger.warning(
            f"[StepOrchestrator] DEPRECATED: StepOrchestrator is deprecated. "
            f"Use UnifiedReActEngine with PlanModeAdapter instead."
        )
        logger.info(f"[StepOrchestrator] Initialized for session {session_id} with model {model_name or 'default'}")
    
    def stop(self):
        """
        Request stop of execution.
        """
        self._stop_requested = True
        if self._streaming_task and not self._streaming_task.done():
            self._streaming_task.cancel()
            logger.info(f"[StepOrchestrator] Cancelled streaming task for session {self.session_id}")
        logger.info(f"[StepOrchestrator] Stop requested for session {self.session_id}")
    
    def _load_tools(self) -> List:
        """
        Load all available tools for step execution.
        
        Returns:
            List of LangChain tools
        """
        tools = []
        try:
            # Load workspace tools (file search, etc.)
            workspace_tools = get_workspace_tools()
            tools.extend(workspace_tools)
            
            # Load sheets tools
            sheets_tools = get_sheets_tools()
            tools.extend(sheets_tools)
            
            # Load gmail tools
            gmail_tools = get_gmail_tools()
            tools.extend(gmail_tools)
            
            # Load calendar tools
            calendar_tools = get_calendar_tools()
            tools.extend(calendar_tools)
            
            # Load slides tools (Google Slides / –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏)
            slides_tools = get_slides_tools()
            tools.extend(slides_tools)
            
            # Load docs tools (Google Docs / –î–æ–∫—É–º–µ–Ω—Ç—ã)
            docs_tools = get_docs_tools()
            tools.extend(docs_tools)
            
            # Load 1C tools
            from src.mcp_tools.onec_tools import get_onec_tools
            onec_tools = get_onec_tools()
            tools.extend(onec_tools)
            
            # Load Project Lad tools
            from src.mcp_tools.projectlad_tools import get_projectlad_tools
            projectlad_tools = get_projectlad_tools()
            tools.extend(projectlad_tools)
            
            # Remove duplicates by name
            seen_names = set()
            unique_tools = []
            for tool in tools:
                if tool.name not in seen_names:
                    seen_names.add(tool.name)
                    unique_tools.append(tool)
            
            logger.info(f"[StepOrchestrator] Loaded {len(unique_tools)} tools for step execution")
            return unique_tools
        except Exception as e:
            logger.error(f"[StepOrchestrator] Failed to load tools: {e}")
            return []
    
    def _build_open_files_context(self, context: ConversationContext) -> Optional[str]:
        """
        Build context string for currently open files in workspace panel.
        Uses FileContextResolver for unified priority handling.
        
        Args:
            context: Conversation context
            
        Returns:
            Context string or None if no open files
        """
        open_files = context.get_open_files() if hasattr(context, 'get_open_files') else []
        
        if not open_files:
            return None
        
        # Use FileContextResolver for consistent context building
        resolver = FileContextResolver()
        
        # Get attached files from context
        attached_files = getattr(context, 'uploaded_files', {}) or {}
        
        # Get workspace folder config
        workspace_folder = None
        try:
            from src.utils.config_loader import get_config
            config = get_config()
            workspace_config_path = config.config_dir / "workspace_config.json"
            if workspace_config_path.exists():
                workspace_config = json.loads(workspace_config_path.read_text())
                folder_id = workspace_config.get("folder_id")
                folder_name = workspace_config.get("folder_name")
                if folder_id:
                    workspace_folder = {"folder_id": folder_id, "folder_name": folder_name}
        except Exception:
            pass
        
        # Build unified context with priorities
        return resolver.build_context_string(attached_files, open_files, workspace_folder)
    
    def _is_simple_generative_task(self, user_request: str) -> bool:
        """
        Check if task is a simple generative task (writing text, creating content).
        These tasks don't need verbose reasoning.
        
        Args:
            user_request: User's request
            
        Returns:
            True if task is simple generative
        """
        import re
        request_lower = user_request.lower().strip()
        
        # Simple generative patterns (similar to TaskClassifier)
        simple_generative_patterns = [
            r"–Ω–∞–ø–∏—à–∏\s+(–∫—Ä–∞—Ç–∫–æ–µ\s+)?(–ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ|—Å—Ç–∏—Ö|—Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ|—à—É—Ç–∫—É|–∞–Ω–µ–∫–¥–æ—Ç|—Å–æ–æ–±—â–µ–Ω–∏–µ|—Ç–µ–∫—Å—Ç|–ø–∏—Å—å–º–æ\s+—Å\s+–ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ–º|—Ö–æ—Ö–∫—É|—Ö–∞–π–∫—É)",
            r"–ø—Ä–∏–¥—É–º–∞–π\s+(–ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ|—Å—Ç–∏—Ö|—à—É—Ç–∫—É|–Ω–∞–∑–≤–∞–Ω–∏–µ|–∏–º—è|–∏—Å—Ç–æ—Ä–∏—é)",
            r"—Å–æ—á–∏–Ω–∏\s+(—Å—Ç–∏—Ö|–ø–µ—Å–Ω—é|–∏—Å—Ç–æ—Ä–∏—é|—Å–∫–∞–∑–∫—É|—Ö–æ—Ö–∫—É|—Ö–∞–π–∫—É)",
            r"write\s+(a\s+)?(greeting|poem|joke|message|story|haiku)",
            r"–∞–¥–∞–ø—Ç–∏—Ä—É–π.*\s+(–∏\s+|–∞\s+–ø–æ—Ç–æ–º\s+|–ø–æ—Ç–æ–º\s+)?(–Ω–∞–ø–∏—à–∏|–ø—Ä–∏–¥—É–º–∞–π|–¥–æ–±–∞–≤—å)",
            r"(–Ω–∞–ø–∏—à–∏|—Å–æ–∑–¥–∞–π).*\s+(–∏\s+|–∞\s+–ø–æ—Ç–æ–º\s+|–ø–æ—Ç–æ–º\s+)?(–Ω–∞–ø–∏—à–∏|–ø—Ä–∏–¥—É–º–∞–π|–¥–æ–±–∞–≤—å)\s+\w+",
        ]
        
        for pattern in simple_generative_patterns:
            try:
                if re.search(pattern, request_lower):
                    logger.info(f"[StepOrchestrator] Simple generative task detected: {pattern}")
                    return True
            except Exception as e:
                logger.warning(f"[StepOrchestrator] Error matching pattern {pattern}: {e}")
                continue
        
        return False
    
    def _create_llm_with_thinking(self, enable_thinking: bool = True, budget_tokens: int = 5000) -> BaseChatModel:
        """
        Create LLM instance with optional extended thinking.
        
        Args:
            enable_thinking: If False, create LLM without thinking even if model supports it
            budget_tokens: Token budget for thinking (if enabled). Lower values = shorter reasoning
        
        Returns:
            BaseChatModel instance
        """
        from src.utils.config_loader import get_config
        from langchain_anthropic import ChatAnthropic
        from langchain_openai import ChatOpenAI
        
        config_model_name = self.model_name or "claude-sonnet-4-5"
        config = get_config()
        
        try:
            from src.agents.model_factory import get_available_models, create_llm
            available_models = get_available_models()
            
            if config_model_name in available_models:
                model_config = available_models[config_model_name]
                provider = model_config.get("provider")
                
                # If thinking should be disabled, use create_llm (which handles thinking based on model config)
                if not enable_thinking:
                    logger.info(f"[StepOrchestrator] Creating LLM without thinking for model {config_model_name}")
                    # For Anthropic models, create without thinking parameter
                    if provider == "anthropic":
                        if model_config.get("supports_reasoning") and model_config.get("reasoning_type") == "extended_thinking":
                            # Create LLM without thinking even though model supports it
                            llm = ChatAnthropic(
                                model=model_config["model_id"],
                                api_key=config.anthropic_api_key,
                                streaming=True,
                                temperature=1.0,  # Standard temperature without thinking
                            )
                            return llm
                    # For OpenAI o1 models, we can't disable reasoning (it's built-in)
                    # So we'll still use create_llm which handles it properly
                    return create_llm(config_model_name)
                
                # Enable thinking if model supports it
                if model_config.get("supports_reasoning"):
                    reasoning_type = model_config.get("reasoning_type")
                    
                    if provider == "anthropic" and reasoning_type == "extended_thinking":
                        # Create LLM with extended thinking
                        llm = ChatAnthropic(
                            model=model_config["model_id"],
                            api_key=config.anthropic_api_key,
                            streaming=True,
                            temperature=1,  # Required for extended thinking
                            thinking={
                                "type": "enabled",
                                "budget_tokens": budget_tokens
                            }
                        )
                        return llm
                    elif provider == "openai" and reasoning_type == "native":
                        # OpenAI o1 models have native reasoning (built-in, can't disable)
                        # Use create_llm which handles it properly
                        return create_llm(config_model_name)
            
            # Fallback: use create_llm for models without thinking support
            return create_llm(config_model_name)
        except Exception as e:
            logger.error(f"[StepOrchestrator] Failed to create LLM: {e}")
            # Fallback to default model
            try:
                from src.agents.model_factory import create_llm
                return create_llm(config.default_model)
            except Exception as e2:
                logger.error(f"[StepOrchestrator] Failed to create fallback LLM: {e2}")
                raise
    
    async def execute(
        self,
        user_request: str,
        mode: str,
        context: ConversationContext,
        file_ids: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        file_ids = file_ids or []
        """
        Execute multi-step workflow.
        
        Args:
            user_request: User's request
            mode: "plan_and_confirm" or "plan_and_execute"
            context: Conversation context
            
        Returns:
            Execution result with final status
        """
        try:
            # Step 1: Generate plan
            logger.info(f"[StepOrchestrator] Generating plan for request: {user_request[:100]}")
            plan_result = await self._generate_plan(user_request, context, file_ids)
            
            # Check if stop was requested during plan generation
            if self._stop_requested:
                logger.info(f"[StepOrchestrator] Stop requested during plan generation")
                await self.ws_manager.send_event(
                    self.session_id,
                    "workflow_stopped",
                    {
                        "reason": "–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º",
                        "step": 0,
                        "remaining_steps": 0
                    }
                )
                return {
                    "status": "stopped",
                    "message": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"
                }
            
            plan_text = plan_result["plan"]
            plan_steps = plan_result["steps"]
            
            self._plan_text = plan_text
            self._plan_steps = plan_steps
            self._confirmation_id = str(uuid4())
            
            # NEW: If plan has only 1 step, skip plan display and execute directly
            if len(plan_steps) == 1:
                logger.info(f"[StepOrchestrator] Single-step plan detected, executing directly without showing plan")
                
                # Check if stop was requested before executing single step
                if self._stop_requested:
                    logger.info(f"[StepOrchestrator] Stop requested before single-step execution")
                    await self.ws_manager.send_event(
                        self.session_id,
                        "workflow_stopped",
                        {
                            "reason": "–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º",
                            "step": 0,
                            "remaining_steps": 1
                        }
                    )
                    return {
                        "status": "stopped",
                        "message": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º",
                        "plan": plan_text,
                        "steps": plan_steps
                    }
                
                # Set current step index for file_preview events
                self._current_step_index = 1
                # Execute step directly
                step_result = await self._execute_step(
                    1,
                    plan_steps[0],
                    user_request,
                    plan_text,
                    plan_steps,
                    [],
                    context,
                    file_ids
                )
                # Clear current step index
                self._current_step_index = None
                
                # Send final result directly (skip separate generation)
                await self.ws_manager.send_event(
                    self.session_id,
                    "final_result_start",
                    {}
                )
                await self.ws_manager.send_event(
                    self.session_id,
                    "final_result_complete",
                    {"content": step_result}
                )
                
                # Save to context
                if hasattr(context, 'add_message'):
                    context.add_message("assistant", step_result)
                
                return {
                    "status": "completed",
                    "steps": [{"step": 1, "title": plan_steps[0], "result": step_result}],
                    "plan": plan_text,
                    "single_step": True
                }
            
            # Send plan_generated event (only for multi-step plans)
            await self.ws_manager.send_event(
                self.session_id,
                "plan_generated",
                {
                    "plan": plan_text,
                    "steps": plan_steps,
                    "confirmation_id": self._confirmation_id
                }
            )
            
            # Check if stop was requested after plan generation
            if self._stop_requested:
                logger.info(f"[StepOrchestrator] Stop requested after plan generation")
                await self.ws_manager.send_event(
                    self.session_id,
                    "workflow_stopped",
                    {
                        "reason": "–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º",
                        "step": 0,
                        "remaining_steps": len(plan_steps)
                    }
                )
                return {
                    "status": "stopped",
                    "message": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º",
                    "plan": plan_text,
                    "steps": plan_steps
                }
            
            # Step 2: Handle confirmation based on mode
            if mode == "plan_and_confirm":
                # Send awaiting_confirmation event
                await self.ws_manager.send_event(
                    self.session_id,
                    "awaiting_confirmation",
                    {}
                )
                
                # Store confirmation in context
                context.add_pending_confirmation(self._confirmation_id, {
                    "plan": plan_text,
                    "steps": plan_steps,
                    "mode": mode
                })
                
                # Wait for confirmation (will be triggered by confirm_plan() or reject_plan())
                self._confirmation_event = asyncio.Event()
                confirmation_timeout = 300  # 5 minutes timeout
                try:
                    # Wait for confirmation with periodic stop checks
                    while not self._confirmation_event.is_set():
                        if self._stop_requested:
                            logger.info(f"[StepOrchestrator] Stop requested during confirmation wait")
                            await self.ws_manager.send_event(
                                self.session_id,
                                "workflow_stopped",
                                {
                                    "reason": "–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º",
                                    "step": 0,
                                    "remaining_steps": len(plan_steps)
                                }
                            )
                            # Reset confirmation state
                            self._confirmation_event = None
                            self._confirmation_result = None
                            return {
                                "status": "stopped",
                                "message": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"
                            }
                        
                        try:
                            await asyncio.wait_for(
                                self._confirmation_event.wait(),
                                timeout=0.5  # Check every 0.5 seconds
                            )
                            break
                        except asyncio.TimeoutError:
                            # Continue checking _stop_requested
                            continue
                except asyncio.TimeoutError:
                    logger.warning(f"[StepOrchestrator] Confirmation timeout for session {self.session_id}")
                    await self.ws_manager.send_event(
                        self.session_id,
                        "error",
                        {"message": "Confirmation timeout. Plan execution cancelled."}
                    )
                    # Reset confirmation state
                    self._confirmation_event = None
                    self._confirmation_result = None
                    
                    return {
                        "status": "timeout",
                        "message": "Confirmation timeout"
                    }
                
                # Check if stop was requested after confirmation
                if self._stop_requested:
                    logger.info(f"[StepOrchestrator] Stop requested after confirmation")
                    await self.ws_manager.send_event(
                        self.session_id,
                        "workflow_stopped",
                        {
                            "reason": "–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º",
                            "step": 0,
                            "remaining_steps": len(plan_steps)
                        }
                    )
                    # Reset confirmation state
                    self._confirmation_event = None
                    self._confirmation_result = None
                    return {
                        "status": "stopped",
                        "message": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"
                    }
                
                if not self._confirmation_result:
                    # Plan was rejected
                    await self.ws_manager.send_event(
                        self.session_id,
                        "error",
                        {"message": "Plan rejected by user"}
                    )
                    # Reset confirmation state
                    self._confirmation_event = None
                    self._confirmation_result = None
                    
                    return {
                        "status": "rejected",
                        "message": "Plan rejected"
                    }
            
            # Step 3: Execute steps sequentially
            logger.info(f"[StepOrchestrator] Executing {len(plan_steps)} steps")
            step_results = []
            
            for step_index, step_title in enumerate(plan_steps, start=1):
                # Check if stop was requested before starting step
                if self._stop_requested:
                    logger.info(f"[StepOrchestrator] Stop requested, stopping execution before step {step_index}")
                    await self.ws_manager.send_event(
                        self.session_id,
                        "workflow_stopped",
                        {
                            "reason": "–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º",
                            "step": step_index - 1,
                            "remaining_steps": len(plan_steps) - step_index + 1
                        }
                    )
                    break
                
                try:
                    # Check again before sending step_start event
                    if self._stop_requested:
                        logger.info(f"[StepOrchestrator] Stop requested before step {step_index} start event")
                        await self.ws_manager.send_event(
                            self.session_id,
                            "workflow_stopped",
                            {
                                "reason": "–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º",
                                "step": step_index - 1,
                                "remaining_steps": len(plan_steps) - step_index + 1
                            }
                        )
                        break
                    
                    # Send step_start event
                    await self.ws_manager.send_event(
                        self.session_id,
                        "step_start",
                        {
                            "step": step_index,
                            "title": step_title
                        }
                    )
                    # Set current step index for file_preview events
                    self._current_step_index = step_index
                    # Execute step with streaming
                    step_result = await self._execute_step(
                        step_index,
                        step_title,
                        user_request,
                        plan_text,
                        plan_steps,
                        step_results,
                        context,
                        file_ids
                    )
                    # Add step result
                    step_results.append({
                        "step": step_index,
                        "title": step_title,
                        "result": step_result
                    })
                    
                    # Send step_complete event
                    await self.ws_manager.send_event(
                        self.session_id,
                        "step_complete",
                        {
                            "step": step_index
                        }
                    )
                    
                    # Check if stop was requested after sending step_complete
                    if self._stop_requested:
                        logger.info(f"[StepOrchestrator] Stop requested after step {step_index} completion")
                        await self.ws_manager.send_event(
                            self.session_id,
                            "workflow_stopped",
                            {
                                "reason": "–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º",
                                "step": step_index,
                                "remaining_steps": len(plan_steps) - step_index
                            }
                        )
                        break
                    
                    # Check if step requires user help (critical error)
                    if "üõë –¢–†–ï–ë–£–ï–¢–°–Ø –ü–û–ú–û–©–¨ –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø" in step_result:
                        logger.warning(f"[StepOrchestrator] Step {step_index} requires user help, stopping execution")
                        await self.ws_manager.send_event(
                            self.session_id,
                            "workflow_paused",
                            {
                                "reason": "–®–∞–≥ —Ç—Ä–µ–±—É–µ—Ç –ø–æ–º–æ—â–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è",
                                "step": step_index,
                                "remaining_steps": len(plan_steps) - step_index
                            }
                        )
                        # Stop executing remaining steps
                        break
                    
                    # Check if stop was requested after step execution (before adding result)
                    if self._stop_requested:
                        logger.info(f"[StepOrchestrator] Stop requested after step {step_index} execution")
                        await self.ws_manager.send_event(
                            self.session_id,
                            "workflow_stopped",
                            {
                                "reason": "–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º",
                                "step": step_index,
                                "remaining_steps": len(plan_steps) - step_index
                            }
                        )
                        break
                    
                except Exception as e:
                    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                    from src.utils.exceptions import AgentError
                    if isinstance(e, AgentError) and "–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ" in str(e).lower():
                        logger.info(f"[StepOrchestrator] Stop exception caught for step {step_index}")
                        await self.ws_manager.send_event(
                            self.session_id,
                            "workflow_stopped",
                            {
                                "reason": "–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º",
                                "step": step_index,
                                "remaining_steps": len(plan_steps) - step_index
                            }
                        )
                        break
                    # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏, –ø—Ä–æ–±—Ä–æ—Å–∏—Ç—å –¥–∞–ª—å—à–µ
                    logger.error(f"[StepOrchestrator] Error executing step {step_index}: {e}")
                    await self.ws_manager.send_event(
                        self.session_id,
                        "error",
                        {
                            "message": f"Error in step {step_index}: {str(e)}"
                        }
                    )
                    raise# Step 4: Check if stop was requested before sending workflow_complete
            if self._stop_requested:
                logger.info(f"[StepOrchestrator] Stop requested, workflow not completed")
                # Reset confirmation state
                self._confirmation_event = None
                self._confirmation_result = None
                return {
                    "status": "stopped",
                    "message": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º",
                    "steps": step_results,
                    "plan": plan_text,
                    "confirmation_id": self._confirmation_id
                }
            
            # Send workflow_complete event only if not stopped
            await self.ws_manager.send_event(
                self.session_id,
                "workflow_complete",
                {}
            )
            
            logger.info(f"[StepOrchestrator] Workflow completed successfully")
            
            # Generate and stream final result (events are sent from _generate_final_result)
            try:
                await self._generate_final_result(
                    user_request,
                    plan_text,
                    step_results,
                    context
                )
                
                logger.info(f"[StepOrchestrator] Final result generated and streamed")
            except Exception as e:
                logger.error(f"[StepOrchestrator] Error generating final result: {e}")
                # Don't fail the whole workflow if result generation fails
                # Just log the error and continue
            
            # Reset confirmation state
            self._confirmation_event = None
            self._confirmation_result = None
            
            return {
                "status": "completed",
                "steps": step_results,
                "plan": plan_text,
                "confirmation_id": self._confirmation_id
            }
            
        except Exception as e:
            logger.error(f"[StepOrchestrator] Error in execute: {e}", exc_info=True)
            await self.ws_manager.send_event(
                self.session_id,
                "error",
                {"message": str(e)}
            )
            # Reset confirmation state on error
            self._confirmation_event = None
            self._confirmation_result = None
            raise
    
    async def _generate_plan(
        self,
        user_request: str,
        context: ConversationContext,
        file_ids: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        file_ids = file_ids or []
        
        # Helper function to create HumanMessage with file attachments
        def create_message_with_files(text: str, files: List[Dict[str, Any]]) -> HumanMessage:
            """
Create HumanMessage with text and optional file attachments."""
            content_parts = [{"type": "text", "text": text}]
            
            # Add image files as image_url blocks
            for file in files:
                if file.get("type", "").startswith("image/") and "data" in file:
                    content_parts.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:{file.get('media_type', file.get('type', 'image/png'))};base64,{file['data']}"
                        }
                    })
                elif "text" in file:  # PDF text content
                    # Append PDF text to the text content (escape braces in file text)
                    filename = _escape_braces_for_fstring(str(file.get('filename', 'document.pdf')))
                    file_text = _escape_braces_for_fstring(str(file['text']))
                    # Use string concatenation instead of f-string to avoid issues
                    content_parts[0]["text"] += "\n\n[–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ " + filename + "]:\n" + file_text
            
            # If we have multiple content parts or complex content, use list format
            if len(content_parts) > 1 or any(part.get("type") != "text" for part in content_parts):
                return HumanMessage(content=content_parts)
            else:
                # Simple text message
                return HumanMessage(content=text)
        
        # Get file data from context
        files_data = []
        for file_id in file_ids:
            file_data = context.get_file(file_id)
            if file_data:
                files_data.append(file_data)
        
        # Build uploaded files information text (PRIORITY #1)
        uploaded_files_info = ""
        if files_data:
            uploaded_files_info = "\n\nüìé –ó–ê–ì–†–£–ñ–ï–ù–ù–´–ï –§–ê–ô–õ–´ (–ü–†–ò–û–†–ò–¢–ï–¢ #1):\n"
            uploaded_files_info += "‚ö†Ô∏è –í–ê–ñ–ù–û: –¢–µ–∫—Å—Ç —ç—Ç–∏—Ö —Ñ–∞–π–ª–æ–≤ –£–ñ–ï –≤–∫–ª—é—á–µ–Ω –≤ —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∏–∂–µ!\n"
            uploaded_files_info += "‚ö†Ô∏è –ù–ï –∏—â–∏ —ç—Ç–∏ —Ñ–∞–π–ª—ã –≤ Google –î–∏—Å–∫ - –∏—Å–ø–æ–ª—å–∑—É–π —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è!\n\n"
            for i, file in enumerate(files_data, 1):
                filename = file.get('filename', 'unknown')
                file_type = file.get('type', '')
                if file_type.startswith('image/'):
                    uploaded_files_info += f"{i}. –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {filename} (–≤–∫–ª—é—á–µ–Ω–æ –≤ —Å–æ–æ–±—â–µ–Ω–∏–µ)\n"
                elif file_type == 'application/pdf' and 'text' in file:
                    uploaded_files_info += f"{i}. PDF —Ñ–∞–π–ª: {filename}\n   ‚ö†Ô∏è –¢–ï–ö–°–¢ –§–ê–ô–õ–ê –£–ñ–ï –í–ö–õ–Æ–ß–ï–ù –í –°–û–û–ë–©–ï–ù–ò–ï –ù–ò–ñ–ï! –ò—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é, –ù–ï –∏—â–∏ —Ñ–∞–π–ª –≤ Google –î–∏—Å–∫!\n"
                else:
                    uploaded_files_info += f"{i}. –§–∞–π–ª: {filename} (—Ç–∏–ø: {file_type})\n"
            uploaded_files_info += "\n‚ö†Ô∏è –°–ù–ê–ß–ê–õ–ê –∏—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (—Ç–µ–∫—Å—Ç —É–∂–µ –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏), –ü–û–¢–û–ú —É–∂–µ –∏—â–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ Google –î–∏—Å–∫!\n"
        
        """
        Generate detailed execution plan using Claude.
        
        Args:
            user_request: User's request
            context: Conversation context
            
        Returns:
            Dict with "plan" (text) and "steps" (list of step titles)
        """
        # Use dynamic planning prompt
        system_prompt = build_planning_prompt()

        # Prepare messages with file attachments if any
        # Include uploaded files info FIRST, then user request
        # Escape user_request to avoid f-string syntax errors if it contains braces
        escaped_user_request = _escape_braces_for_fstring(user_request)
        plan_request_text = uploaded_files_info + f"\n\n–°–æ–∑–¥–∞–π –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞:\n\n{escaped_user_request}"
        user_message = create_message_with_files(plan_request_text, files_data)
        
        # Add entity context to system prompt (combine into single SystemMessage to avoid "multiple non-consecutive system messages" error)
        has_entity_context = False
        if hasattr(context, 'entity_memory') and context.entity_memory.has_recent_entities():
            entity_context = context.entity_memory.to_context_string()
            if entity_context:
                system_prompt += f"""

–ö–û–ù–¢–ï–ö–°–¢ –£–ü–û–ú–Ø–ù–£–¢–´–• –û–ë–™–ï–ö–¢–û–í:

{entity_context}

‚ÑπÔ∏è –ò–ù–°–¢–†–£–ö–¶–ò–Ø: –û–±—ä–µ–∫—Ç—ã –≤—ã—à–µ —É–∂–µ –¥–æ—Å—Ç—É–ø–Ω—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–∏–∞–ª–æ–≥–∞. –ü—Ä–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ —à–∞–≥–æ–≤:
- –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –Ω–∏—Ö (—á–µ—Ä–µ–∑ "—ç—Ç–æ—Ç", "—Ç–æ—Ç", "—Ç–∞–∫–æ–π") - –∏—Å–ø–æ–ª—å–∑—É–π –∏—Ö –Ω–∞–ø—Ä—è–º—É—é
- –ù–ï –ø–ª–∞–Ω–∏—Ä—É–π –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —É–∂–µ –µ—Å—Ç—å –≤ —Å–ø–∏—Å–∫–µ –≤—ã—à–µ
- –ò—Å–ø–æ–ª—å–∑—É–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (ID) –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –ø—Ä—è–º–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –Ω–∏–º"""
                has_entity_context = True
        
        # Add open files context to system prompt (if available)
        open_files_context = self._build_open_files_context(context)
        if open_files_context:
            system_prompt += f"\n\n{open_files_context}"
        
        messages = [
            SystemMessage(content=system_prompt),  # Single SystemMessage with all system info
        ]
        
        # Get context for planning (includes last 10 messages)
        recent_messages = context.get_context_for_planning()
        
        # Check if task is simple generative - disable reasoning for such tasks
        is_simple_generative = self._is_simple_generative_task(user_request)
        
        # Create LLM for planning (without thinking for simple generative tasks, reduced budget for complex tasks)
        # Use lower budget_tokens (3000) for complex tasks to reduce verbose reasoning
        budget_tokens = 3000  # Reduced from 5000 to minimize verbose reasoning
        planning_llm = self._create_llm_with_thinking(
            enable_thinking=not is_simple_generative,
            budget_tokens=budget_tokens
        )
        if is_simple_generative:
            logger.info(f"[StepOrchestrator] Simple generative task detected, planning without thinking")
        else:
            logger.info(f"[StepOrchestrator] Complex task detected, planning with thinking (budget: {budget_tokens} tokens)")
        
        # Check if LLM uses extended thinking (for message formatting)
        uses_extended_thinking = False
        try:
            from src.agents.model_factory import get_available_models
            available_models = get_available_models()
            config_model_name = self.model_name or "claude-sonnet-4-5"
            if config_model_name in available_models:
                model_config = available_models[config_model_name]
                # Only check if thinking is enabled AND task is not simple generative
                if not is_simple_generative and model_config.get("supports_reasoning") and model_config.get("reasoning_type") == "extended_thinking":
                    uses_extended_thinking = True
        except:
            pass
        
        # Add all recent messages (user + assistant) for context (UPDATED)
        # This enables understanding of references and continuation
        for msg in recent_messages:
            role = msg.get("role")
            content = msg.get("content", "")
            if role == "user":
                # Add user messages for context (NEW - previously skipped)
                messages.append(HumanMessage(content=content))
            elif role == "assistant":
                # Include assistant messages for context to enable references to previous responses
                # For extended thinking models, wrap as HumanMessage to avoid thinking block errors and "multiple system messages" error
                if uses_extended_thinking:
                    # Wrap assistant message as HumanMessage with label to preserve context while avoiding API errors
                    # Using HumanMessage instead of SystemMessage to avoid "multiple non-consecutive system messages" error
                    messages.append(HumanMessage(
                        content=f"[–ü—Ä–µ–¥—ã–¥—É—â–∏–π –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞]:\n{content}"
                    ))
                else:
                    # Normal AIMessage for non-extended-thinking models
                    messages.append(AIMessage(content=content))
        
        # Add current user request message
        messages.append(user_message)
        try:
            # Check if stop was requested before starting streaming
            if self._stop_requested:
                logger.info(f"[StepOrchestrator] Stop requested before plan generation")
                return {
                    "plan": "Execution plan (stopped)",
                    "steps": ["Execute request"]
                }
            
            # Stream LLM response with thinking (if enabled)
            accumulated_thinking = ""
            accumulated_response = ""
            
            # –°–æ–∑–¥–∞—Ç—å –∑–∞–¥–∞—á—É –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –µ—ë –æ—Ç–º–µ–Ω–∏—Ç—å
            async def stream_plan():
                nonlocal accumulated_thinking, accumulated_response
                async for chunk in planning_llm.astream(messages):
                    if self._stop_requested:
                        break
                    
                    if hasattr(chunk, 'content'):
                        content = chunk.content
                        
                        # Handle list of content blocks (Claude format with thinking)
                        if isinstance(content, list):
                            for block in content:
                                block_type = block.get("type") if isinstance(block, dict) else getattr(block, "type", None)
                                
                                if block_type == "thinking":
                                    # Extract thinking text
                                    thinking_text = block.get("thinking", "") if isinstance(block, dict) else getattr(block, "thinking", "")
                                    if thinking_text:
                                        accumulated_thinking += thinking_text
                                        # Stream thinking chunk to UI
                                        await self.ws_manager.send_event(
                                            self.session_id,
                                            "plan_thinking_chunk",
                                            {"content": thinking_text}
                                        )
                                
                                elif block_type == "text":
                                    # Extract response text
                                    text_content = block.get("text", "") if isinstance(block, dict) else getattr(block, "text", "")
                                    if text_content:
                                        accumulated_response += text_content
                        
                        # Handle simple string content
                        elif isinstance(content, str):
                            accumulated_response += content
            
            # –ó–∞–ø—É—Å—Ç–∏—Ç—å –∑–∞–¥–∞—á—É –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Å—ã–ª–∫—É
            self._streaming_task = asyncio.create_task(stream_plan())
            
            try:
                await self._streaming_task
            except asyncio.CancelledError:
                logger.info(f"[StepOrchestrator] Plan streaming cancelled")
                if self._stop_requested:
                    return {
                        "plan": "Execution plan (stopped)",
                        "steps": ["Execute request"]
                    }
                raise
            finally:
                self._streaming_task = None
            
            # Check if stop was requested after streaming
            if self._stop_requested:
                logger.info(f"[StepOrchestrator] Stop requested after plan generation, returning partial plan")
                # Return partial plan if we have something
                if accumulated_response:
                    try:
                        json_match = re.search(r'\{[\s\S]*\}', accumulated_response)
                        if json_match:
                            json_str = json_match.group(0)
                            plan_data = json.loads(json_str)
                            plan_text = plan_data.get("plan", "Execution plan (partial)")
                            steps = plan_data.get("steps", [])
                        else:
                            plan_text = accumulated_response[:500] if accumulated_response else "Execution plan (partial)"
                            steps = ["Execute request"]
                    except:
                        plan_text = accumulated_response[:500] if accumulated_response else "Execution plan (partial)"
                        steps = ["Execute request"]
                else:
                    plan_text = "Execution plan (stopped)"
                    steps = ["Execute request"]
                
                return {
                    "plan": plan_text,
                    "steps": steps
                }
            
            # Parse JSON response
            response_text = accumulated_response
            
            # Try to extract JSON from response (handle cases where LLM adds markdown code blocks)
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                json_str = json_match.group(0)
            else:
                json_str = response_text
            
            plan_data = json.loads(json_str)
            
            plan_text = plan_data.get("plan", "Execution plan")
            steps = plan_data.get("steps", [])
            
            if not steps:
                # Fallback: create a simple single-step plan
                steps = ["Execute request"]
            
            logger.info(f"[StepOrchestrator] Generated plan with {len(steps)} steps (thinking: {len(accumulated_thinking)} chars)")
            
            # Send event to stop plan thinking streaming before returning plan
            # This ensures the thinking block collapses immediately when plan is generated
            await self.ws_manager.send_event(
                self.session_id,
                "plan_thinking_complete",
                {}
            )
            
            return {
                "plan": plan_text,
                "steps": steps
            }
            
        except Exception as e:
            logger.error(f"[StepOrchestrator] Error generating plan: {e}")
            # Fallback plan
            # Escape user_request to avoid f-string syntax errors
            escaped_user_request = _escape_braces_for_fstring(user_request)
            return {
                "plan": f"Execute: {escaped_user_request}",
                "steps": [f"Step 1: {escaped_user_request}"]
            }
    
    async def _execute_step(
        self,
        step_index: int,
        step_title: str,
        user_request: str,
        plan_text: str,
        all_steps: List[str],
        previous_results: List[Dict[str, Any]],
        context: ConversationContext,
        file_ids: Optional[List[str]] = None
    ) -> str:
        file_ids = file_ids or []
        
        # Helper function to create HumanMessage with file attachments
        def create_message_with_files(text: str, files: List[Dict[str, Any]]) -> HumanMessage:
            """
Create HumanMessage with text and optional file attachments."""
            content_parts = [{"type": "text", "text": text}]
            text_was_modified = False
            
            # Add image files as image_url blocks
            for file in files:
                if file.get("type", "").startswith("image/") and "data" in file:
                    content_parts.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:{file.get('media_type', file.get('type', 'image/png'))};base64,{file['data']}"
                        }
                    })
                elif "text" in file:  # PDF text content
                    # Append PDF text to the text content (escape braces in file text)
                    filename = _escape_braces_for_fstring(str(file.get('filename', 'document.pdf')))
                    file_text = _escape_braces_for_fstring(str(file['text']))
                    # Use string concatenation instead of f-string to avoid issues
                    content_parts[0]["text"] += "\n\n[–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ " + filename + "]:\n" + file_text
                    text_was_modified = True
            
            # Always use content_parts if text was modified (PDF added) or if we have images
            if text_was_modified or len(content_parts) > 1 or any(part.get("type") != "text" for part in content_parts):
                return HumanMessage(content=content_parts)
            else:
                # Simple text message (no files)
                return HumanMessage(content=text)
        
        """
        Execute a single step with streaming thinking and response.
        
        Args:
            step_index: Step number (1-based)
            step_title: Title of the step
            user_request: Original user request
            plan_text: Overall plan description
            all_steps: All step titles
            previous_results: Results from previous steps
            context: Conversation context
            
        Returns:
            Step execution result
        """
        # Get available capabilities for dynamic prompt generation
        try:
            capabilities = await get_available_capabilities()
        except Exception as e:
            logger.warning(f"[StepOrchestrator] Could not get capabilities: {e}, using defaults")
            capabilities = {"enabled_servers": [], "tools_by_category": {}}
        
        # Get file data from context
        files_data = []
        for file_id in file_ids:
            file_data = context.get_file(file_id)
            if file_data:
                files_data.append(file_data)
        
        # Build uploaded files information text (PRIORITY #1)
        uploaded_files_info = ""
        if files_data:
            uploaded_files_info = "üìé –ó–ê–ì–†–£–ñ–ï–ù–ù–´–ï –§–ê–ô–õ–´ (–ü–†–ò–û–†–ò–¢–ï–¢ #1):\n"
            uploaded_files_info += "‚ö†Ô∏è –í–ê–ñ–ù–û: –¢–µ–∫—Å—Ç —ç—Ç–∏—Ö —Ñ–∞–π–ª–æ–≤ –£–ñ–ï –≤–∫–ª—é—á–µ–Ω –≤ —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∏–∂–µ!\n"
            uploaded_files_info += "‚ö†Ô∏è –ù–ï –∏—â–∏ —ç—Ç–∏ —Ñ–∞–π–ª—ã –≤ Google –î–∏—Å–∫ - –∏—Å–ø–æ–ª—å–∑—É–π —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è!\n\n"
            for i, file in enumerate(files_data, 1):
                filename = file.get('filename', 'unknown')
                file_type = file.get('type', '')
                if file_type.startswith('image/'):
                    uploaded_files_info += f"{i}. –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {filename} (–≤–∫–ª—é—á–µ–Ω–æ –≤ —Å–æ–æ–±—â–µ–Ω–∏–µ)\n"
                elif file_type == 'application/pdf' and 'text' in file:
                    uploaded_files_info += f"{i}. PDF —Ñ–∞–π–ª: {filename}\n   ‚ö†Ô∏è –¢–ï–ö–°–¢ –§–ê–ô–õ–ê –£–ñ–ï –í–ö–õ–Æ–ß–ï–ù –í –°–û–û–ë–©–ï–ù–ò–ï –ù–ò–ñ–ï! –ò—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é, –ù–ï –∏—â–∏ —Ñ–∞–π–ª –≤ Google –î–∏—Å–∫!\n"
                else:
                    uploaded_files_info += f"{i}. –§–∞–π–ª: {filename} (—Ç–∏–ø: {file_type})\n"
            uploaded_files_info += "\n‚ö†Ô∏è –°–ù–ê–ß–ê–õ–ê –∏—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (—Ç–µ–∫—Å—Ç —É–∂–µ –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏), –ü–û–¢–û–ú —É–∂–µ –∏—â–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ Google –î–∏—Å–∫!\n\n"
        
        # Read workspace folder configuration for context (PRIORITY #2)
        workspace_folder_info = None
        try:
            from src.utils.config_loader import get_config
            config = get_config()
            workspace_config_path = config.config_dir / "workspace_config.json"
            
            if workspace_config_path.exists():
                workspace_config = json.loads(workspace_config_path.read_text())
                folder_id = workspace_config.get("folder_id")
                folder_name = workspace_config.get("folder_name")
                if folder_id:
                    workspace_folder_info = f"""üìÅ GOOGLE –î–ò–°–ö –ü–ê–ü–ö–ê (–ü–†–ò–û–†–ò–¢–ï–¢ #2):
‚ö†Ô∏è –ü–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–π —ç—Ç—É –ø–∞–ø–∫—É Google –î–∏—Å–∫:
  –ù–∞–∑–≤–∞–Ω–∏–µ: {folder_name}
  ID: {folder_id}
  
  –ò—Å–ø–æ–ª—å–∑—É–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π –ø–∞–ø–∫–æ–π, —É–∫–∞–∑—ã–≤–∞—è folder_id={folder_id}
"""
        except Exception as e:
            logger.warning(f"[StepOrchestrator] Could not read workspace config: {e}")
        
        # Build context for this step
        # PRIORITY ORDER: 1) Uploaded files, 2) Google Drive folder, 3) Original request and plan
        step_context = ""
        if uploaded_files_info:
            step_context += uploaded_files_info + "\n"
        if workspace_folder_info:
            step_context += workspace_folder_info + "\n"
        # Escape user_request and plan_text to avoid f-string syntax errors
        escaped_user_request = _escape_braces_for_fstring(user_request)
        escaped_plan_text = _escape_braces_for_fstring(plan_text)# Use string concatenation instead of f-string to avoid issues
        step_context += "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å: " + escaped_user_request + "\n\n"
        step_context += "–û–±—â–∏–π –ø–ª–∞–Ω: " + escaped_plan_text + "\n"
        
        step_context += """
–í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ —à–∞–≥–∏:
"""
        for i, result in enumerate(previous_results, start=1):
            # Escape braces in result content to avoid f-string syntax errors
            result_title = _escape_braces_for_fstring(str(result['title']))
            result_content = _escape_braces_for_fstring(str(result['result']))
            # Use string concatenation instead of f-string to avoid issues
            step_context += "  " + str(i) + ". " + result_title + ": " + result_content + "\n"
        
        # Escape step_title to avoid f-string syntax errors
        escaped_step_title = _escape_braces_for_fstring(str(step_title))
        # Use string concatenation instead of f-string to avoid issues
        step_context += "\n–¢–µ–∫—É—â–∏–π —à–∞–≥ (" + str(step_index) + " –∏–∑ " + str(len(all_steps)) + "): " + escaped_step_title + "\n\n"
        step_context += "–í—ã–ø–æ–ª–Ω–∏ —ç—Ç–æ—Ç —à–∞–≥. –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å —á–µ—Ç–∫–∏–π, –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç."

        # Build dynamic system prompt based on available capabilities
        system_prompt = build_step_executor_prompt(capabilities, workspace_folder_info)
        
        # Add conversation history for context (NEW - enables access to previous assistant responses)
        # This allows the model to see its previous responses (e.g., generated text, analysis)
        recent_messages = context.get_context_for_planning()  # Get last 10 messages
        
        # Check if LLM uses extended thinking
        uses_extended_thinking = False
        try:
            from src.agents.model_factory import get_available_models
            available_models = get_available_models()
            config_model_name = self.model_name or "claude-sonnet-4-5"
            if config_model_name in available_models:
                model_config = available_models[config_model_name]
                if model_config.get("supports_reasoning") and model_config.get("reasoning_type") == "extended_thinking":
                    uses_extended_thinking = True
        except:
            pass
        
        # Add entity context to system prompt (combine into single SystemMessage to avoid "multiple non-consecutive system messages" error)
        if hasattr(context, 'entity_memory') and context.entity_memory.has_recent_entities():
            entity_context = context.entity_memory.to_context_string()
            if entity_context:
                system_prompt += f"""

–ö–û–ù–¢–ï–ö–°–¢ –£–ü–û–ú–Ø–ù–£–¢–´–• –û–ë–™–ï–ö–¢–û–í:

{entity_context}

‚ÑπÔ∏è –ò–ù–°–¢–†–£–ö–¶–ò–Ø: –û–±—ä–µ–∫—Ç—ã –≤—ã—à–µ —É–∂–µ –¥–æ—Å—Ç—É–ø–Ω—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–∏–∞–ª–æ–≥–∞. –ü—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —à–∞–≥–∞:
- –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –Ω–∏—Ö (—á–µ—Ä–µ–∑ "—ç—Ç–æ—Ç", "—Ç–æ—Ç", "—Ç–∞–∫–æ–π") - –∏—Å–ø–æ–ª—å–∑—É–π –∏—Ö –Ω–∞–ø—Ä—è–º—É—é
- –ù–ï –ø–ª–∞–Ω–∏—Ä—É–π –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —É–∂–µ –µ—Å—Ç—å –≤ —Å–ø–∏—Å–∫–µ –≤—ã—à–µ
- –ò—Å–ø–æ–ª—å–∑—É–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (ID) –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –ø—Ä—è–º–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –Ω–∏–º"""
        
        # Add open files context to system prompt (CRITICAL: enables agent to use already open files)
        open_files_context = self._build_open_files_context(context)
        if open_files_context:
            system_prompt += f"\n\n{open_files_context}"
        
        # Prepare messages with file attachments if any (files_data already loaded above)
        step_message = create_message_with_files(step_context, files_data)
        messages = [
            SystemMessage(content=system_prompt),  # Single SystemMessage with all system info
        ]
        
        # Add recent messages from conversation history
        assistant_messages_count = 0
        user_messages_count = 0
        for msg in recent_messages:
            role = msg.get("role")
            content = msg.get("content", "")
            if role == "user":
                messages.append(HumanMessage(content=content))
                user_messages_count += 1
            elif role == "assistant":
                # Include assistant messages for context to enable references to previous responses
                assistant_messages_count += 1
                if uses_extended_thinking:
                    # Wrap as HumanMessage with label to preserve context while avoiding API errors
                    # Using HumanMessage instead of SystemMessage to avoid "multiple non-consecutive system messages" error
                    assistant_context = f"[–ü—Ä–µ–¥—ã–¥—É—â–∏–π –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞]:\n{content}"
                    messages.append(HumanMessage(content=assistant_context))
                    
                else:
                    messages.append(AIMessage(content=content))
                    
        
        # Add step message with context
        messages.append(step_message)
        
        # Check if stop was requested before starting streaming
        if self._stop_requested:
            logger.info(f"[StepOrchestrator] Stop requested before step {step_index} execution")
            # –í—ã–±—Ä–æ—Å–∏—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã –ø—Ä–µ—Ä–≤–∞—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ü–∏–∫–ª–∞ —à–∞–≥–æ–≤
            from src.utils.exceptions import AgentError
            raise AgentError("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        
        # Stream the response with thinking
        accumulated_thinking = ""
        accumulated_response = ""
        
        try:
            # –°–æ–∑–¥–∞—Ç—å –∑–∞–¥–∞—á—É –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –µ—ë –æ—Ç–º–µ–Ω–∏—Ç—å
            async def stream_step():
                nonlocal accumulated_thinking, accumulated_response
                # Stream the response with thinking support
                # LangChain ChatAnthropic with thinking enabled streams thinking and text separately
                # Use llm_with_tools to enable tool calling
                # Collect all chunks to handle tool calls
                all_chunks = []
                async for chunk in self.llm_with_tools.astream(messages):
                    if self._stop_requested:
                        break
                    all_chunks.append(chunk)
                    
                    # Process chunk - chunks are AIMessageChunk objects
                    if hasattr(chunk, 'content'):
                        content = chunk.content
                        
                        # Handle list of content blocks (Claude format with thinking)
                        if isinstance(content, list):
                            for block in content:
                                # Handle both dict and object formats
                                block_type = block.get("type") if isinstance(block, dict) else getattr(block, "type", None)
                                
                                if block_type == "thinking":
                                    # For thinking blocks, text is in 'thinking' key, not 'text'!
                                    thinking_text = block.get("thinking", "") if isinstance(block, dict) else getattr(block, "thinking", "")
                                    if thinking_text:
                                        accumulated_thinking += thinking_text
                                        # Send thinking chunk
                                        await self.ws_manager.send_event(
                                            self.session_id,
                                            "thinking_chunk",
                                            {"content": thinking_text}
                                        )
                                elif block_type == "text":
                                    # For text blocks, text is in 'text' key
                                    text_content = block.get("text", "") if isinstance(block, dict) else getattr(block, "text", "")
                                    if text_content:
                                        accumulated_response += text_content
                                        # Send response chunk
                                        await self.ws_manager.send_event(
                                            self.session_id,
                                            "response_chunk",
                                            {"content": text_content}
                                        )
                        # Handle string content (fallback)
                        elif isinstance(content, str) and content:
                            accumulated_response += content
                            await self.ws_manager.send_event(
                                self.session_id,
                                "response_chunk",
                                {"content": content}
                            )
                    # Fallback: if chunk doesn't have content attribute, try to extract text
                    elif hasattr(chunk, 'text'):
                        text = chunk.text
                        if text:
                            accumulated_response += text
                            await self.ws_manager.send_event(
                                self.session_id,
                                "response_chunk",
                                {"content": text}
                            )
                    else:
                        # Last resort: convert to string
                        chunk_str = str(chunk)
                        if chunk_str and chunk_str not in ['', 'None']:
                            accumulated_response += chunk_str
                            await self.ws_manager.send_event(
                                self.session_id,
                                "response_chunk",
                                {"content": chunk_str}
                            )
            
            # –ó–∞–ø—É—Å—Ç–∏—Ç—å –∑–∞–¥–∞—á—É –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Å—ã–ª–∫—É
            self._streaming_task = asyncio.create_task(stream_step())
            
            try:
                await self._streaming_task
            except asyncio.CancelledError:
                logger.info(f"[StepOrchestrator] Step {step_index} streaming cancelled")
                if self._stop_requested:
                    # –í—ã–±—Ä–æ—Å–∏—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã –ø—Ä–µ—Ä–≤–∞—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ü–∏–∫–ª–∞ —à–∞–≥–æ–≤
                    from src.utils.exceptions import AgentError
                    raise AgentError("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                raise
            finally:
                self._streaming_task = None
            
            # Check if stop was requested after streaming
            if self._stop_requested:
                logger.info(f"[StepOrchestrator] Stop requested after step {step_index} streaming")
                # –í—ã–±—Ä–æ—Å–∏—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã –ø—Ä–µ—Ä–≤–∞—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ü–∏–∫–ª–∞ —à–∞–≥–æ–≤
                from src.utils.exceptions import AgentError
                raise AgentError("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            
            logger.info(f"[StepOrchestrator] Step {step_index} completed, response length: {len(accumulated_response)}")
            
            # After streaming, check if we need to execute tools
            # Get full response to check for tool calls
            try:
                full_response = await self.llm_with_tools.ainvoke(messages)
                
                if hasattr(full_response, 'tool_calls') and full_response.tool_calls:
                    # Execute tool calls
                    from langchain_core.messages import ToolMessage
                    tool_messages = []
                    for tool_call in full_response.tool_calls:
                        tool_name = tool_call.get("name") if isinstance(tool_call, dict) else getattr(tool_call, "name", None)
                        tool_args = tool_call.get("args") if isinstance(tool_call, dict) else getattr(tool_call, "args", {})
                        tool_id = tool_call.get("id") if isinstance(tool_call, dict) else getattr(tool_call, "id", None)
                        
                        if tool_name:
                            # Find and execute tool
                            tool = next((t for t in self.tools if t.name == tool_name), None)
                            if tool:
                                try:
                                    # Send intermediate message about tool execution
                                    tool_display_name = self._get_tool_display_name(tool_name, tool_args)
                                    intermediate_msg = f"{tool_display_name}..."
                                    # Update step response with intermediate message
                                    if self.session_id:
                                        await self.ws_manager.send_event(
                                            self.session_id,
                                            "response_chunk",
                                            {"content": intermediate_msg + "\n"}
                                        )
                                    
                                    result = await tool.ainvoke(tool_args)
                                    tool_messages.append(ToolMessage(content=str(result), tool_call_id=tool_id or tool_name))
                                    # Don't add raw tool results to user-facing response - LLM will generate user-friendly message
                                    
                                    # NEW: Extract entities from tool result and add to entity memory
                                    if hasattr(context, 'add_entity_from_tool_result'):
                                        try:
                                            context.add_entity_from_tool_result(
                                                tool_name=tool_name,
                                                tool_result=result,
                                                turn_number=len(context.messages)
                                            )
                                            logger.debug(f"[StepOrchestrator] Extracted entities from tool {tool_name}")
                                        except Exception as e:
                                            logger.warning(f"[StepOrchestrator] Failed to extract entities from tool result: {e}")
                                    
                                    # NEW: Handle workspace events (send sheets_action, docs_action, etc.)
                                    try:
                                        await self._handle_workspace_events(tool_name, str(result), tool_args)
                                    except Exception as e:
                                        logger.warning(f"[StepOrchestrator] Failed to handle workspace events: {e}")
                                except Exception as e:
                                    logger.error(f"[StepOrchestrator] Tool {tool_name} execution failed: {e}")
                                    tool_messages.append(ToolMessage(content=f"–û—à–∏–±–∫–∞: {str(e)}", tool_call_id=tool_id or tool_name))
                    
                    # If we have tool results, call LLM again with tool results
                    if tool_messages:
                        messages_with_tools = messages + [full_response] + tool_messages
                        final_response = await self.llm_with_tools.ainvoke(messages_with_tools)
                        
                        if hasattr(final_response, 'content'):
                            final_text = final_response.content
                            # Handle list content (Anthropic format)
                            if isinstance(final_text, list):
                                # Extract text from list of blocks
                                text_parts = []
                                for block in final_text:
                                    if isinstance(block, dict):
                                        if block.get('type') == 'text':
                                            text_parts.append(block.get('text', ''))
                                    elif hasattr(block, 'text'):
                                        text_parts.append(block.text)
                                    elif isinstance(block, str):
                                        text_parts.append(block)
                                final_text = ''.join(text_parts) if text_parts else str(final_text)
                            elif not isinstance(final_text, str):
                                final_text = str(final_text)
                            
                            if final_text and final_text not in accumulated_response:
                                accumulated_response += final_text
                        
                        # Check if final_response has more tool calls (recursive tool calling)
                        # IMPORTANT: Anthropic returns tool_calls as a list, but it might be empty list
                        tool_calls_to_execute = None
                        try:
                            if hasattr(final_response, 'tool_calls'):
                                tool_calls_raw = final_response.tool_calls
                                # Check if it's a non-empty list
                                if tool_calls_raw and len(tool_calls_raw) > 0:
                                    tool_calls_to_execute = tool_calls_raw
                        except Exception as e:
                            logger.error(f"[StepOrchestrator] Error checking tool_calls: {e}")
                        
                        if tool_calls_to_execute and len(tool_calls_to_execute) > 0:
                            # Execute additional tool calls recursively
                            for tool_call in tool_calls_to_execute:
                                tool_name = tool_call.get("name") if isinstance(tool_call, dict) else getattr(tool_call, "name", None)
                                tool_args = tool_call.get("args") if isinstance(tool_call, dict) else getattr(tool_call, "args", {})
                                tool_id = tool_call.get("id") if isinstance(tool_call, dict) else getattr(tool_call, "id", None)
                                
                                if tool_name:
                                    tool = next((t for t in self.tools if t.name == tool_name), None)
                                    if tool:
                                        try:
                                            # Send intermediate message about tool execution
                                            tool_display_name = self._get_tool_display_name(tool_name, tool_args)
                                            intermediate_msg = f"{tool_display_name}..."
                                            if self.session_id:
                                                await self.ws_manager.send_event(
                                                    self.session_id,
                                                    "response_chunk",
                                                    {"content": intermediate_msg + "\n"}
                                                )
                                            
                                            result = await tool.ainvoke(tool_args)
                                            tool_messages.append(ToolMessage(content=str(result), tool_call_id=tool_id or tool_name))
                                            # Don't add raw tool results to user-facing response - LLM will generate user-friendly message
                                            
                                            # NEW: Handle workspace events (send sheets_action, docs_action, etc.)
                                            try:
                                                await self._handle_workspace_events(tool_name, str(result), tool_args)
                                            except Exception as e:
                                                logger.warning(f"[StepOrchestrator] Failed to handle workspace events: {e}")
                                        except Exception as e:
                                            logger.error(f"[StepOrchestrator] Recursive tool {tool_name} execution failed: {e}")
                                            tool_messages.append(ToolMessage(content=f"–û—à–∏–±–∫–∞: {str(e)}", tool_call_id=tool_id or tool_name))
                            
                            # If we got more tool results, call LLM one more time (but limit recursion)
                            if len(tool_messages) > len([tm for tm in tool_messages if hasattr(tm, 'tool_call_id')]):
                                # Limit to 3 iterations to avoid infinite loops
                                if len([m for m in messages_with_tools if isinstance(m, ToolMessage)]) < 10:
                                    messages_with_tools = messages_with_tools + [final_response] + tool_messages[-len(final_response.tool_calls):]
                                    final_response = await self.llm_with_tools.ainvoke(messages_with_tools)
                                    if hasattr(final_response, 'content'):
                                        final_text = final_response.content
                                        if final_text and final_text not in accumulated_response:
                                            accumulated_response += final_text
            except Exception as e:
                logger.error(f"[StepOrchestrator] Error checking/executing tool calls: {e}")
            
            # Check if response contains user assistance request
            assistance_request = self._parse_assistance_request(accumulated_response)
            
            # If no explicit request, check for missing tool scenario
            if not assistance_request:
                assistance_request = self._detect_missing_tool_scenario(accumulated_response)
            
            if assistance_request:
                # Remove JSON from response before returning (to avoid showing raw JSON in chat)
                accumulated_response = self._remove_assistance_request_json(accumulated_response)
                
                # Pause execution and request user assistance
                await self._request_user_assistance(assistance_request, step_index, context)
                # Wait for user response
                await self._wait_for_user_assistance()
                # Get selected option and continue
                if self._user_assistance_result:
                    selected_option = self._user_assistance_result
                    # Add selected option context to response for continuation
                    accumulated_response += f"\n\n–í—ã–±—Ä–∞–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º: {selected_option.get('label', '')}\n"
                    if 'data' in selected_option:
                        accumulated_response += f"–î–∞–Ω–Ω—ã–µ –≤—ã–±–æ—Ä–∞: {json.dumps(selected_option['data'], ensure_ascii=False)}\n"
            else:
                # Even if no assistance request, clean any JSON that might be present
                accumulated_response = self._remove_assistance_request_json(accumulated_response)
            
            return accumulated_response
            
        except Exception as e:
            logger.error(f"[StepOrchestrator] Error streaming step {step_index}: {e}")
            # Escape braces in error message to avoid f-string syntax errors
            error_str = str(e)
            escaped_error_str = _escape_braces_for_fstring(error_str)
            # Use .format() instead of f-string to avoid issues with escaped content
            error_msg = "Error executing step: {error}".format(error=escaped_error_str)
            await self.ws_manager.send_event(
                self.session_id,
                "error",
                {"message": "Error executing step: {error}".format(error=error_str)}  # Use original for event, not escaped
            )
            raise
    
    def _needs_final_result_generation(
        self,
        user_request: str,
        step_results: List[Dict[str, Any]]
    ) -> bool:
        """
        Determine if separate final result generation is needed.
        
        Returns False if last step result can be used directly.
        
        Args:
            user_request: Original user request
            step_results: Results from all executed steps
            
        Returns:
            True if separate final result generation is needed, False otherwise
        """
        if not isinstance(step_results, list):
            logger.error(f"[StepOrchestrator] step_results is not a list: {type(step_results)}")
            return True  # Safe default: generate result
        if not isinstance(user_request, str):
            logger.error(f"[StepOrchestrator] user_request is not a string: {type(user_request)}")
            return True  # Safe default: generate result
        request_lower = user_request.lower()
        
        # 1. Single step - last result IS the final result
        if len(step_results) <= 1:
            return False
        
        # 2. Generative tasks - result is the generated content
        generative_indicators = [
            "–Ω–∞–ø–∏—à–∏", "—Å–æ–∑–¥–∞–π —Ç–µ–∫—Å—Ç", "–ø—Ä–∏–¥—É–º–∞–π", "—Å–æ—á–∏–Ω–∏",
            "write", "compose", "create a message"
        ]
        for indicator in generative_indicators:
            if indicator in request_lower:
                return False
        
        # 2.5. Generative tasks with structured data (tables, lists) - last step result is final
        structured_data_indicators = [
            "—Ç–∞–±–ª–∏—Ü", "—Å–ø–∏—Å–æ–∫", "—Ç–∞–±–ª–∏—á–∫—É", "—Ç–∞–±–ª–∏—Ü—É", 
            "table", "list", "—Å–ø–∏—Å–∫–∞", "—Ç–∞–±–ª–∏—Ü—ã"
        ]
        is_structured_data_task = any(indicator in request_lower for indicator in structured_data_indicators)
        
        if is_structured_data_task:
            # Check if last step result contains structured data (table markdown, list format)
            if step_results:
                last_result = step_results[-1].get("result", "")
                last_result_lower = last_result.lower()
                
                # Check for table indicators in result
                table_indicators = ["|", "---", "—Ç–∞–±–ª–∏—Ü", "—Ç–∞–±–ª–∏—Ü–∞", "table"]
                has_table = any(indicator in last_result_lower or indicator in last_result for indicator in table_indicators)
                
                # Check for list indicators
                list_indicators = ["- ", "* ", "1. ", "‚Ä¢ ", "—Å–ø–∏—Å–æ–∫"]
                has_list = any(indicator in last_result for indicator in list_indicators)
                
                # If last step contains structured data, it's the final result
                if has_table or has_list:
                    logger.info(f"[StepOrchestrator] Last step contains structured data (table/list), using it as final result")
                    return False
        
        # 3. Check if steps performed actions that need summarizing
        action_indicators = [
            "–æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ", "—Å–æ–∑–¥–∞–Ω–æ", "–æ–±–Ω–æ–≤–ª–µ–Ω–æ", "—É–¥–∞–ª–µ–Ω–æ",
            "sent", "created", "updated", "deleted",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞", "Tool result"
        ]
        action_count = 0
        for step in step_results:
            result = step.get("result", "")
            for indicator in action_indicators:
                if indicator in result:
                    action_count += 1
                    break
        
        # If multiple actions were performed, need summary
        if action_count > 1:
            return True
        
        # 4. Multiple data-gathering steps - need consolidation (but skip if structured data)
        # If it's a structured data task and last step has the data, don't generate
        if len(step_results) > 2:
            # Double-check: if this is structured data task and last step has it, use it
            if is_structured_data_task and step_results:
                last_result = step_results[-1].get("result", "")
                # If last result is substantial (more than 200 chars), it's likely complete
                if len(last_result) > 200:
                    return False
            return True
        
        # Default: no generation needed
        return False
    
    async def _generate_final_result(
        self,
        user_request: str,
        plan_text: str,
        step_results: List[Dict[str, Any]],
        context: ConversationContext
    ) -> None:
        """
        Generate and stream final result summary after all steps are completed.
        Results are sent via WebSocket events (final_result_start, final_result_chunk, final_result_complete).
        
        Args:
            user_request: Original user request
            plan_text: Overall plan description
            step_results: Results from all executed steps
            context: Conversation context
        """
        # NEW: Check if separate final result generation is needed
        needs_generation = self._needs_final_result_generation(user_request, step_results)
        if not needs_generation:
            # Use last step result directly as final result
            if not step_results:
                logger.error(f"[StepOrchestrator] step_results is empty, cannot use last result")
                last_result = ""
            else:
                last_result = step_results[-1].get("result", "")
            logger.info(f"[StepOrchestrator] Using last step result as final result (no generation needed)")
            
            await self.ws_manager.send_event(
                self.session_id,
                "final_result_start",
                {}
            )
            await self.ws_manager.send_event(
                self.session_id,
                "final_result_complete",
                {"content": last_result}
            )
            
            # Save to context
            if hasattr(context, 'add_message'):
                context.add_message("assistant", last_result)
            return
        
        # Build summary of all steps - format as context data without step numbering
        # This prevents the model from copying "–®–∞–≥ N:" format into the final answer
        steps_summary = ""
        for step_result in step_results:
            step_content = step_result.get("result", "")
            # Extract slice outside f-string to avoid syntax errors
            step_content_preview = step_content[:1000] if len(step_content) > 1000 else step_content
            # Escape braces in content BEFORE checking length to avoid f-string syntax errors
            escaped_step_content = _escape_braces_for_fstring(str(step_content_preview))
            # Build lines using string concatenation (NOT f-string) to avoid issues with trailing braces and ellipsis
            # Don't include "–®–∞–≥ N:" or "–†–µ–∑—É–ª—å—Ç–∞—Ç:" - just the content
            if len(step_content) > 1000:
                # Use string concatenation to safely add ellipsis without f-string parsing issues
                result_line = escaped_step_content + "...\n\n"
            else:
                result_line = escaped_step_content + "\n\n"
            steps_summary += result_line# Create prompt for final result generation
        system_prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º. –°–æ–∑–¥–∞–π –ø—Ä—è–º–æ–π –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

‚ö†Ô∏è –í–ê–ñ–ù–û: –í–°–ï –æ—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞ –†–£–°–°–ö–û–ú —è–∑—ã–∫–µ! ‚ö†Ô∏è

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞
3. –°–æ–∑–¥–∞—Ç—å –ø–æ–Ω—è—Ç–Ω—ã–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–ø—Ä—è–º—É—é –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
4. –ù–ï —É–ø–æ–º–∏–Ω–∞–π —à–∞–≥–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è, –ø–æ–ø—ã—Ç–∫–∏, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞–±–æ—Ç—ã
5. –ù–ï —Å–æ–∑–¥–∞–≤–∞–π –æ—Ç—á–µ—Ç –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ - —Å–æ–∑–¥–∞–π –∏–º–µ–Ω–Ω–æ –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
- –ò—Å–ø–æ–ª—å–∑—É–π Markdown –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–∂–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç **—Ç–µ–∫—Å—Ç**, —Å–ø–∏—Å–∫–∏, —ç–º–æ–¥–∑–∏ –∏ —Ç.–¥.)
- –ü—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- –ö–ª—é—á–µ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –±—ã–ª–∞ –∑–∞–ø—Ä–æ—à–µ–Ω–∞
- –ï—Å–ª–∏ –Ω—É–∂–Ω–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —á—Ç–µ–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É–π —Å–ø–∏—Å–∫–∏, –∑–∞–≥–æ–ª–æ–≤–∫–∏, –≤—ã–¥–µ–ª–µ–Ω–∏–µ)

–ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –∏ –æ—Ç–≤–µ—á–∞–π –∏–º–µ–Ω–Ω–æ –Ω–∞ —Ç–æ, —á—Ç–æ —Å–ø—Ä–æ—Å–∏–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å."""

        # Escape all content to avoid f-string syntax errors
        escaped_user_request = _escape_braces_for_fstring(user_request)
        escaped_plan_text = _escape_braces_for_fstring(plan_text)
        escaped_steps_summary = _escape_braces_for_fstring(steps_summary)
        # Use .format() instead of f-string to avoid issues with escaped content containing {...}
        user_prompt = """–ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_request}

–î–∞–Ω–Ω—ã–µ, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞:
{steps_summary}

–°–æ–∑–¥–∞–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–ø—Ä—è–º—É—é –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –µ–≥–æ –∑–∞–ø—Ä–æ—Å. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞. –ù–ï —É–ø–æ–º–∏–Ω–∞–π –ø—Ä–æ—Ü–µ—Å—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è, —à–∞–≥–∏ –∏–ª–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ - –ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—Ç—å –Ω–∞ –∑–∞–ø—Ä–æ—Å.""".format(
            user_request=escaped_user_request,
            steps_summary=escaped_steps_summary
        )
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        try:
            # Send final_result_start event
            await self.ws_manager.send_event(
                self.session_id,
                "final_result_start",
                {}
            )
            
            # Stream final result using LLM
            accumulated_result = ""
            async for chunk in self.llm.astream(messages):
                if self._stop_requested:
                    break
                
                # Extract text content from chunk
                if hasattr(chunk, 'content'):
                    content = chunk.content
                    
                    # Handle list of content blocks (Claude format)
                    if isinstance(content, list):
                        for block in content:
                            block_type = block.get("type") if isinstance(block, dict) else getattr(block, "type", None)
                            
                            if block_type == "text":
                                # Extract text from text blocks
                                text_content = block.get("text", "") if isinstance(block, dict) else getattr(block, "text", "")
                                if text_content:
                                    accumulated_result += text_content
                                    # Send chunk
                                    await self.ws_manager.send_event(
                                        self.session_id,
                                        "final_result_chunk",
                                        {"content": accumulated_result}
                                    )
                    # Handle string content (fallback)
                    elif isinstance(content, str) and content:
                        accumulated_result += content
                        await self.ws_manager.send_event(
                            self.session_id,
                            "final_result_chunk",
                            {"content": accumulated_result}
                        )
                # Fallback: if chunk has text attribute
                elif hasattr(chunk, 'text'):
                    text = chunk.text
                    if text:
                        accumulated_result += text
                        await self.ws_manager.send_event(
                            self.session_id,
                            "final_result_chunk",
                            {"content": accumulated_result}
                        )
            
            final_result = accumulated_result.strip()
            logger.info(f"[StepOrchestrator] Generated and streamed final result, length: {len(final_result)}")
            
            # Save final result to context (NEW - enables access to previous responses)
            if hasattr(context, 'add_message'):
                context.add_message("assistant", final_result)
                logger.info(f"[StepOrchestrator] Final result saved to context, context now has {len(context.messages)} messages")
            
            # Send final_result_complete event
            await self.ws_manager.send_event(
                self.session_id,
                "final_result_complete",
                {"content": final_result}
            )
            
        except Exception as e:
            logger.error(f"[StepOrchestrator] Error generating final result: {e}")
            # Fallback: create simple final answer from step results
            # Combine all step results into a single answer without step numbering
            combined_results = []
            for r in step_results:
                r_result = r.get('result', '')
                if r_result and r_result.strip():
                    # Take first 300 chars of each result to avoid too long fallback
                    r_result_preview = r_result[:300] if len(r_result) > 300 else r_result
                    escaped_r_result = _escape_braces_for_fstring(str(r_result_preview))
                    combined_results.append(escaped_r_result)
            
            # Join results with newlines, add ellipsis if truncated
            results_text = "\n\n".join(combined_results)
            if any(len(r.get('result', '')) > 300 for r in step_results):
                results_text += "\n\n..."
            
            # Send fallback result
            fallback_result = results_text if results_text.strip() else "–ó–∞–ø—Ä–æ—Å –≤—ã–ø–æ–ª–Ω–µ–Ω."
            await self.ws_manager.send_event(
                self.session_id,
                "final_result_start",
                {}
            )
            await self.ws_manager.send_event(
                self.session_id,
                "final_result_complete",
                {"content": fallback_result}
            )
    
    def confirm_plan(self) -> None:
        """
        Confirm the pending plan for execution.
        Should be called when user approves the plan.
        """
        logger.info(f"[StepOrchestrator] Plan confirmed for session {self.session_id}")
        self._confirmation_result = True
        if self._confirmation_event:
            self._confirmation_event.set()
    
    def reject_plan(self) -> None:
        """
        Reject the pending plan.
        Should be called when user rejects the plan.
        """
        logger.info(f"[StepOrchestrator] Plan rejected for session {self.session_id}")
        self._confirmation_result = False
        if self._confirmation_event:
            self._confirmation_event.set()
    
    def get_confirmation_id(self) -> Optional[str]:
        """
Get the confirmation ID for the current plan."""
        return self._confirmation_id
    
    def update_pending_plan(self, updated_plan: Dict[str, Any]) -> None:
        """
        Update the pending plan before execution.
        
        Args:
            updated_plan: Dictionary with "plan" (text) and "steps" (list of step titles)
        """
        logger.info(f"[StepOrchestrator] Updating pending plan for session {self.session_id}")
        self._plan_text = updated_plan.get("plan", self._plan_text)
        self._plan_steps = updated_plan.get("steps", self._plan_steps)
        
        # Send updated plan event to frontend
        asyncio.create_task(
            self.ws_manager.send_event(
                self.session_id,
                "plan_updated",
                {
                    "plan": self._plan_text,
                    "steps": self._plan_steps,
                    "confirmation_id": self._confirmation_id
                }
            )
        )
    
    def resolve_user_assistance(self, assistance_id: str, user_response: str) -> None:
        """
        Resolve a user assistance request with user's response.
        Parses the response and finds the selected option.
        
        Args:
            assistance_id: Assistance request ID
            user_response: User's response text (can be number, ordinal, label, etc.)
        """
        if self._user_assistance_id != assistance_id:
            logger.warning(f"[StepOrchestrator] Assistance ID mismatch: expected {self._user_assistance_id}, got {assistance_id}")
            return
        
        if not self._user_assistance_options:
            logger.warning(f"[StepOrchestrator] No options available for assistance request {assistance_id}")
            return
        
        # Parse user response to find selected option
        selected_option = self._parse_user_selection(user_response, self._user_assistance_options)
        
        if not selected_option:
            logger.warning(f"[StepOrchestrator] Could not parse user selection from response: {user_response}")
            # Try to use first option as fallback
            selected_option = self._user_assistance_options[0] if self._user_assistance_options else None
        
        if selected_option:
            logger.info(f"[StepOrchestrator] User assistance resolved for session {self.session_id}, selected option: {selected_option.get('id', 'unknown')}")
            
            self._user_assistance_result = selected_option
            if self._user_assistance_event:
                self._user_assistance_event.set()
                
        else:
            logger.error(f"[StepOrchestrator] No option selected for assistance request {assistance_id}")
    
    def get_user_assistance_id(self) -> Optional[str]:
        """
        Get the current user assistance request ID.
        
        Returns:
            Assistance ID or None if no pending request
        """
        return self._user_assistance_id
    
    def _detect_missing_tool_scenario(self, response_text: str) -> Optional[Dict[str, Any]]:
        """
        Detect if LLM indicates it cannot find a suitable tool.
        Returns assistance request if detected.
        
        Args:
            response_text: LLM response text
            
        Returns:
            Assistance request dict or None if not detected
        """
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
        patterns = [
            r"–Ω–µ\s+–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç\s+—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ",
            r"–Ω–∏\s+–æ–¥–∏–Ω.*–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç.*–Ω–µ\s+–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç",
            r"–Ω–µ—Ç.*–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç.*–¥–ª—è\s+—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è",
            r"–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.*–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å.*—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å",
            r"–Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ.*–≤—ã–ø–æ–ª–Ω–∏—Ç—å.*—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ",
            r"–Ω–µ\s+–Ω–∞—à–µ–ª.*–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç.*–¥–ª—è",
            r"–Ω–µ—Ç.*–ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ.*–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç",
            r"–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç.*–Ω–µ\s+–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç",
        ]
        
        for pattern in patterns:
            if re.search(pattern, response_text, re.IGNORECASE):
                logger.info(f"[StepOrchestrator] Detected missing tool scenario in response")
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–µ–π—Å—Ç–≤–∏–π
                return {
                    "question": "–°–∏—Å—Ç–µ–º–∞ –Ω–µ –º–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ç—Ä–µ–±—É–µ–º–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏. –ö–∞–∫ –ø–æ—Å—Ç—É–ø–∏—Ç—å?",
                    "options": [
                        {
                            "id": "1",
                            "label": "–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –±–µ–∑ —ç—Ç–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è",
                            "description": "–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –Ω–µ–≤–æ–∑–º–æ–∂–Ω—É—é –æ–ø–µ—Ä–∞—Ü–∏—é –∏ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏",
                            "data": {"action": "skip"}
                        },
                        {
                            "id": "2",
                            "label": "–ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥",
                            "description": "–ù–∞–π—Ç–∏ –æ–±—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã",
                            "data": {"action": "alternative"}
                        },
                        {
                            "id": "3",
                            "label": "–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ",
                            "description": "–ü—Ä–µ—Ä–≤–∞—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –∑–∞–¥–∞—á–∏",
                            "data": {"action": "stop"}
                        }
                    ],
                    "context": {
                        "detected_pattern": pattern,
                        "reason": "missing_tool"
                    }
                }
        
        return None
    
    def _remove_assistance_request_json(self, text: str) -> str:
        """
        Remove JSON user assistance request from text to prevent it from appearing in chat.
        
        Args:
            text: Text that may contain assistance request JSON
            
        Returns:
            Text with assistance request JSON removed
        """
        if "üîç –ó–ê–ü–†–û–° –ü–û–ú–û–©–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø" not in text:
            return text
        
        try:
            # Find all JSON blocks with the marker
            marker_pos = text.find('üîç –ó–ê–ü–†–û–° –ü–û–ú–û–©–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø')
            while marker_pos != -1:
                # Find the opening brace before the marker
                start_pos = text.rfind('{', 0, marker_pos)
                if start_pos != -1:
                    # Find the matching closing brace
                    brace_count = 0
                    end_pos = start_pos
                    for i in range(start_pos, len(text)):
                        if text[i] == '{':
                            brace_count += 1
                        elif text[i] == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                end_pos = i + 1
                                break
                    
                    if end_pos > start_pos:
                        # Remove the JSON block
                        text = text[:start_pos] + text[end_pos:]
                        # Remove any trailing whitespace/newlines
                        text = text.rstrip()
                
                # Find next marker
                marker_pos = text.find('üîç –ó–ê–ü–†–û–° –ü–û–ú–û–©–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø', marker_pos)
                if marker_pos == -1:
                    break
            
            # Also try regex pattern as fallback
            text = re.sub(
                r'\{\s*["\']?üîç\s*–ó–ê–ü–†–û–°\s*–ü–û–ú–û–©–ò\s*–ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø["\']?\s*:\s*\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}\s*\}',
                '',
                text,
                flags=re.DOTALL | re.IGNORECASE
            )
            
            return text.strip()
        except Exception as e:
            logger.debug(f"[StepOrchestrator] Error removing assistance request JSON: {e}")
            return text
    
    def _parse_assistance_request(self, step_result: str) -> Optional[Dict[str, Any]]:
        """
        Parse user assistance request from LLM response.
        Supports both JSON and text formats.
        
        Args:
            step_result: Step execution result text
            
        Returns:
            Parsed assistance request dict or None if not found
        """
        if "üîç –ó–ê–ü–†–û–° –ü–û–ú–û–©–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø" not in step_result:
            return None
        
        try:
            # Try to extract JSON block first - improved regex to match nested JSON
            # Pattern: { "üîç –ó–ê–ü–†–û–° –ü–û–ú–û–©–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø": { ... } }
            # Find ALL JSON blocks with the marker
            json_blocks = []
            marker_pos = step_result.find('üîç –ó–ê–ü–†–û–° –ü–û–ú–û–©–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø')
            while marker_pos != -1:
                # Find the opening brace before the marker
                start_pos = step_result.rfind('{', 0, marker_pos)
                if start_pos != -1:
                    # Find the matching closing brace
                    brace_count = 0
                    end_pos = start_pos
                    for i in range(start_pos, len(step_result)):
                        if step_result[i] == '{':
                            brace_count += 1
                        elif step_result[i] == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                end_pos = i + 1
                                break
                    
                    if end_pos > start_pos:
                        json_blocks.append((start_pos, end_pos))
                
                # Find next marker
                marker_pos = step_result.find('üîç –ó–ê–ü–†–û–° –ü–û–ú–û–©–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø', marker_pos + 1)
            
            # Use the FIRST (and should be only) JSON block
            if json_blocks:
                start_pos, end_pos = json_blocks[0]
                json_str = step_result[start_pos:end_pos]
                
                # Clean up the JSON - remove emoji from key if needed
                json_str = json_str.replace('"üîç –ó–ê–ü–†–û–° –ü–û–ú–û–©–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø"', '"user_assistance_request"')
                json_str = json_str.replace("'üîç –ó–ê–ü–†–û–° –ü–û–ú–û–©–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø'", '"user_assistance_request"')
                
                try:
                    data = json.loads(json_str)
                    assistance_data = data.get("user_assistance_request") or data.get("üîç –ó–ê–ü–†–û–° –ü–û–ú–û–©–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø")
                    if assistance_data and isinstance(assistance_data, dict):
                        # Ensure options is a list and deduplicate by id
                        options_list = assistance_data.get("options", [])
                        if isinstance(options_list, list):
                            # Deduplicate options by id
                            seen_ids = set()
                            unique_options = []
                            for opt in options_list:
                                opt_id = str(opt.get("id", ""))
                                if opt_id and opt_id not in seen_ids:
                                    seen_ids.add(opt_id)
                                    unique_options.append(opt)
                            
                            return {
                                "question": assistance_data.get("question", ""),
                                "options": unique_options,
                                "context": assistance_data.get("context", {})
                            }
                except json.JSONDecodeError as e:
                    logger.debug(f"[StepOrchestrator] Failed to parse JSON: {e}")
            
            # Fallback to regex if brace matching failed
            json_match = re.search(r'\{\s*["\']üîç\s*–ó–ê–ü–†–û–°\s*–ü–û–ú–û–©–ò\s*–ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø["\']\s*:\s*(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})\s*\}', step_result, re.DOTALL | re.IGNORECASE)
            if not json_match:
                # Try simpler pattern
                json_match = re.search(r'\{\s*["\']?üîç\s*–ó–ê–ü–†–û–°\s*–ü–û–ú–û–©–ò\s*–ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø["\']?\s*:\s*(\{.*?\})\s*\}', step_result, re.DOTALL | re.IGNORECASE)
            
            if json_match:
                # Try to extract the full JSON object
                # Find the complete JSON block from { to matching }
                start_pos = step_result.find('{', step_result.find('üîç –ó–ê–ü–†–û–° –ü–û–ú–û–©–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø'))
                if start_pos != -1:
                    brace_count = 0
                    end_pos = start_pos
                    for i in range(start_pos, len(step_result)):
                        if step_result[i] == '{':
                            brace_count += 1
                        elif step_result[i] == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                end_pos = i + 1
                                break
                    
                    if end_pos > start_pos:
                        json_str = step_result[start_pos:end_pos]
                        # Clean up the JSON - remove emoji from key if needed
                        json_str = json_str.replace('"üîç –ó–ê–ü–†–û–° –ü–û–ú–û–©–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø"', '"user_assistance_request"')
                        json_str = json_str.replace("'üîç –ó–ê–ü–†–û–° –ü–û–ú–û–©–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø'", '"user_assistance_request"')
                        try:
                            data = json.loads(json_str)
                            assistance_data = data.get("user_assistance_request") or data.get("üîç –ó–ê–ü–†–û–° –ü–û–ú–û–©–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø")
                            if assistance_data and isinstance(assistance_data, dict):
                                # Ensure options is a list and deduplicate by id
                                options_list = assistance_data.get("options", [])
                                if isinstance(options_list, list):
                                    # Deduplicate options by id
                                    seen_ids = set()
                                    unique_options = []
                                    for opt in options_list:
                                        opt_id = str(opt.get("id", ""))
                                        if opt_id and opt_id not in seen_ids:
                                            seen_ids.add(opt_id)
                                            unique_options.append(opt)
                                    
                                    
                                    return {
                                        "question": assistance_data.get("question", ""),
                                        "options": unique_options,
                                        "context": assistance_data.get("context", {})
                                    }
                        except json.JSONDecodeError as e:
                            logger.debug(f"[StepOrchestrator] Failed to parse JSON: {e}")
        except (AttributeError, Exception) as e:
            logger.debug(f"[StepOrchestrator] Failed to parse JSON assistance request: {e}")
        
        # Fallback: parse text format
        try:
            # Extract question
            question_match = re.search(r'–í–æ–ø—Ä–æ—Å:\s*(.+?)(?:\n|–í–∞—Ä–∏–∞–Ω—Ç—ã:)', step_result, re.IGNORECASE | re.DOTALL)
            question = question_match.group(1).strip() if question_match else "–¢—Ä–µ–±—É–µ—Ç—Å—è –≤—ã–±–æ—Ä –≤–∞—Ä–∏–∞–Ω—Ç–∞"
            
            # Extract options (numbered list)
            options = []
            option_pattern = r'(\d+)\.\s*(.+?)(?=\n\d+\.|\n–£–∫–∞–∂–∏—Ç–µ|$)'
            option_matches = re.finditer(option_pattern, step_result, re.MULTILINE | re.DOTALL)
            
            for match in option_matches:
                option_id = match.group(1)
                option_label = match.group(2).strip()
                options.append({
                    "id": option_id,
                    "label": option_label,
                    "description": "",
                    "data": {}
                })
            
            if options:
                return {
                    "question": question,
                    "options": options,
                    "context": {}
                }
        except Exception as e:
            logger.debug(f"[StepOrchestrator] Failed to parse text assistance request: {e}")
        
        return None
    
    async def _request_user_assistance(
        self,
        assistance_request: Dict[str, Any],
        step_index: int,
        context: ConversationContext
    ) -> None:
        """
        Request user assistance by pausing execution and sending event.
        
        Args:
            assistance_request: Parsed assistance request with question and options
            step_index: Current step index
            context: Conversation context
        """
        self._user_assistance_id = str(uuid4())
        self._user_assistance_event = asyncio.Event()
        self._user_assistance_result = None
        self._user_assistance_context = {
            "step": step_index,
            **assistance_request.get("context", {})
        }
        self._user_assistance_options = assistance_request.get("options", [])
        
        # Send user assistance request event
        await self.ws_manager.send_event(
            self.session_id,
            "user_assistance_request",
            {
                "assistance_id": self._user_assistance_id,
                "question": assistance_request.get("question", "–¢—Ä–µ–±—É–µ—Ç—Å—è –≤—ã–±–æ—Ä –≤–∞—Ä–∏–∞–Ω—Ç–∞"),
                "options": self._user_assistance_options,
                "context": self._user_assistance_context
            }
        )
        
        logger.info(f"[StepOrchestrator] User assistance requested for session {self.session_id}, step {step_index}")
    
    async def _wait_for_user_assistance(self) -> None:
        """
        Wait for user to provide assistance response.
        Similar to confirmation wait logic.
        """
        
        if not self._user_assistance_event:
            return
        
        confirmation_timeout = 300  # 5 minutes timeout
        start_time = time.time()
        
        try:
            while not self._user_assistance_event.is_set():
                elapsed = time.time() - start_time
                
                
                if self._stop_requested:
                    logger.info(f"[StepOrchestrator] Stop requested during user assistance wait")
                    break
                
                if elapsed > confirmation_timeout:
                    logger.warning(f"[StepOrchestrator] User assistance timeout for session {self.session_id}")
                    await self.ws_manager.send_event(
                        self.session_id,
                        "error",
                        {"message": "User assistance timeout. Execution cancelled."}
                    )
                    # Reset assistance state
                    self._user_assistance_event = None
                    self._user_assistance_result = None
                    self._user_assistance_id = None
                    return
                
                try:
                    await asyncio.wait_for(
                        self._user_assistance_event.wait(),
                        timeout=0.5  # Check every 0.5 seconds
                    )
                    break
                except asyncio.TimeoutError:
                    # Continue checking _stop_requested
                    continue
        except Exception as e:
            logger.error(f"[StepOrchestrator] Error waiting for user assistance: {e}")
            raise
    
    def _get_tool_display_name(self, tool_name: str, tool_args: Dict[str, Any]) -> str:
        """Get human-readable display name for tool execution message."""
        # Map tool names to readable actions
        tool_display_map = {
            'workspace_search_files': '–ò—â—É —Ñ–∞–π–ª—ã',
            'workspace_find_and_open_file': '–ò—â—É –∏ –æ—Ç–∫—Ä—ã–≤–∞—é —Ñ–∞–π–ª',
            'workspace_get_file_info': '–ü–æ–ª—É—á–∞—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ',
            'sheets_read_range': '–ß–∏—Ç–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã',
            'sheets_write_range': '–ó–∞–ø–∏—Å—ã–≤–∞—é –¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—Ü—É',
            'sheets_append_rows': '–î–æ–±–∞–≤–ª—è—é —Å—Ç—Ä–æ–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü—É',
            'docs_read': '–ß–∏—Ç–∞—é –¥–æ–∫—É–º–µ–Ω—Ç',
            'docs_create': '–°–æ–∑–¥–∞—é –¥–æ–∫—É–º–µ–Ω—Ç',
            'docs_update': '–û–±–Ω–æ–≤–ª—è—é –¥–æ–∫—É–º–µ–Ω—Ç',
            'slides_create': '–°–æ–∑–¥–∞—é –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—é',
            'slides_create_slide': '–î–æ–±–∞–≤–ª—è—é —Å–ª–∞–π–¥',
            'gmail_search': '–ò—â—É –ø–∏—Å—å–º–∞',
            'gmail_send_email': '–û—Ç–ø—Ä–∞–≤–ª—è—é –ø–∏—Å—å–º–æ',
        }
        
        # Get base action name
        action = tool_display_map.get(tool_name, tool_name.replace('_', ' ').title())
        
        # Add context from tool args if available
        if 'query' in tool_args:
            query = tool_args['query']
            if len(query) < 50:
                return f"{action} '{query}'"
        elif 'title' in tool_args:
            title = tool_args['title']
            if len(title) < 50:
                return f"{action} '{title}'"
        elif 'spreadsheet_id' in tool_args or 'document_id' in tool_args:
            return f"{action}"
        
        return action
    
    async def _handle_workspace_events(
        self,
        tool_name: str,
        result: str,
        tool_args: Dict[str, Any]
    ) -> None:
        """
        Handle workspace panel events based on tool execution results.
        
        Args:
            tool_name: Name of the executed tool
            result: Tool execution result
            tool_args: Tool arguments
        """
        # Handle create_spreadsheet
        if tool_name == "create_spreadsheet":
            logger.info(f"[StepOrchestrator] Processing create_spreadsheet, result length: {len(result)}, result preview: {result[:200]}")
            # Extract spreadsheet ID and URL from result
            # Result format: "Spreadsheet 'title' created successfully. ID: {id}. URL: {url}"
            spreadsheet_id_match = re.search(r'ID:\s*([a-zA-Z0-9_-]+)', result)
            url_match = re.search(r'URL:\s*(https?://[^\s]+)', result)
            title_match = re.search(r"Spreadsheet\s+'([^']+)'", result)
            
            logger.info(f"[StepOrchestrator] Regex matches - ID: {bool(spreadsheet_id_match)}, URL: {bool(url_match)}, Title: {bool(title_match)}")
            
            if spreadsheet_id_match:
                spreadsheet_id = spreadsheet_id_match.group(1)
                
                # Check if we already sent event for this spreadsheet_id (prevent duplicates)
                event_key = f"{self.session_id}:create_spreadsheet:{spreadsheet_id}"
                current_time = time.time()
                if event_key in self._sent_workspace_events:
                    last_sent = self._sent_workspace_events[event_key]
                    # Only skip if sent within last 5 seconds (to allow retries after longer delays)
                    if current_time - last_sent < 5:
                        logger.info(f"[StepOrchestrator] Skipping duplicate sheets_action event for spreadsheet {spreadsheet_id} (sent {current_time - last_sent:.2f}s ago)")
                        return
                
                url = url_match.group(1) if url_match else f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/edit"
                title = title_match.group(1) if title_match else tool_args.get("title", "Google Sheets")
                
                event_data = {
                    "action": "create",
                    "spreadsheet_id": spreadsheet_id,
                    "spreadsheet_url": url,
                    "title": title,
                    "range": None,
                    "description": f"–°–æ–∑–¥–∞–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ '{title}'"
                }
                
                logger.info(f"[StepOrchestrator] Sending sheets_action event: {event_data}")
                await self.ws_manager.send_event(
                    self.session_id,
                    "sheets_action",
                    event_data
                )
                # Mark as sent
                self._sent_workspace_events[event_key] = current_time
                logger.info(f"[StepOrchestrator] Sent sheets_action event for created spreadsheet {spreadsheet_id}")
                
                # Send file_preview event for step log
                if self._current_step_index is not None:
                    await self.ws_manager.send_event(
                        self.session_id,
                        "file_preview",
                        {
                            "step": self._current_step_index,
                            "type": "sheets",
                            "title": title,
                            "fileId": spreadsheet_id,
                            "fileUrl": url
                        }
                    )
                    logger.info(f"[StepOrchestrator] Sent file_preview event for spreadsheet {spreadsheet_id} at step {self._current_step_index}")
            else:
                logger.warning(f"[StepOrchestrator] Failed to extract spreadsheet_id from result: {result[:500]}")
        
        # Handle create_presentation, slides_create, or create_presentation_from_doc (direct MCP call)
        elif tool_name in ("create_presentation", "slides_create", "create_presentation_from_doc"):
            
            logger.info(f"[StepOrchestrator] Processing {tool_name}, result length: {len(result)}, result preview: {result[:200]}")
            
            # Check if result indicates an error (e.g., "ID: None" means presentation was not created)
            if "(ID: None)" in result or "ID: None" in result:
                logger.warning(f"[StepOrchestrator] Presentation creation failed (ID is None), not sending slides_action event")
                return
            
            # Extract presentation ID and URL from result
            # Result format: "Presentation 'title' created successfully (ID: {id}) URL: {url}" or JSON
            try:
                # Try to parse as JSON first
                import json
                result_json = json.loads(result) if isinstance(result, str) else result
                if isinstance(result_json, dict) and "presentationId" in result_json:
                    presentation_id = result_json.get("presentationId")
                    url = result_json.get("url", f"https://docs.google.com/presentation/d/{presentation_id}/edit")
                    title = result_json.get("title", tool_args.get("title", "Google Slides"))
                else:
                    # Fallback to regex parsing
                    presentation_id_match = re.search(r'ID:\s*([a-zA-Z0-9_-]+)', result)
                    url_match = re.search(r'URL:\s*(https?://[^\s]+)', result)
                    title_match = re.search(r"Presentation\s+'([^']+)'", result)
                    
                    if presentation_id_match:
                        presentation_id = presentation_id_match.group(1)
                        url = url_match.group(1) if url_match else f"https://docs.google.com/presentation/d/{presentation_id}/edit"
                        title = title_match.group(1) if title_match else tool_args.get("title", "Google Slides")
                    else:
                        logger.warning(f"[StepOrchestrator] Failed to extract presentation_id from result: {result[:500]}")
                        return
            except json.JSONDecodeError:
                # Not JSON, try regex
                presentation_id_match = re.search(r'ID:\s*([a-zA-Z0-9_-]+)', result)
                url_match = re.search(r'URL:\s*(https?://[^\s]+)', result)
                title_match = re.search(r"Presentation\s+'([^']+)'", result)
                
                if presentation_id_match:
                    presentation_id = presentation_id_match.group(1)
                    url = url_match.group(1) if url_match else f"https://docs.google.com/presentation/d/{presentation_id}/edit"
                    title = title_match.group(1) if title_match else tool_args.get("title", "Google Slides")
                else:
                    logger.warning(f"[StepOrchestrator] Failed to extract presentation_id from result: {result[:500]}")
                    return
            
            
            await self.ws_manager.send_event(
                self.session_id,
                "slides_action",
                {
                    "action": "create",
                    "presentation_id": presentation_id,
                    "presentation_url": url,
                    "title": title,
                    "description": f"–°–æ–∑–¥–∞–Ω–∞ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è '{title}'"
                }
            )
            logger.info(f"[StepOrchestrator] Sent slides_action event for presentation {presentation_id}")
            
            # Send file_preview event for step log
            if self._current_step_index is not None:
                await self.ws_manager.send_event(
                    self.session_id,
                    "file_preview",
                    {
                        "step": self._current_step_index,
                        "type": "slides",
                        "title": title,
                        "fileId": presentation_id,
                        "fileUrl": url
                    }
                )
                logger.info(f"[StepOrchestrator] Sent file_preview event for presentation {presentation_id} at step {self._current_step_index}")
            
    
    @staticmethod
    def _parse_user_selection(user_response: str, options: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """
        Parse user's selection from their response.
        Supports multiple formats: numbers, ordinal words, option labels, option IDs.
        
        Args:
            user_response: User's response text
            options: List of available options
            
        Returns:
            Selected option dict or None if not found
        """
        if not user_response or not options:
            return None
        
        user_response = user_response.strip().lower()
        
        # Try to find by number (1, 2, 3, etc.)
        number_match = re.search(r'^(\d+)', user_response)
        if number_match:
            try:
                index = int(number_match.group(1)) - 1  # Convert to 0-based index
                if 0 <= index < len(options):
                    return options[index]
            except (ValueError, IndexError):
                pass
        
        # Try to find by ordinal words (–ø–µ—Ä–≤—ã–π, –≤—Ç–æ—Ä–æ–π, etc.)
        ordinal_map = {
            "–ø–µ—Ä–≤—ã–π": 0, "–ø–µ—Ä–≤–∞—è": 0, "–ø–µ—Ä–≤–æ–µ": 0, "first": 0,
            "–≤—Ç–æ—Ä–æ–π": 1, "–≤—Ç–æ—Ä–∞—è": 1, "–≤—Ç–æ—Ä–æ–µ": 1, "second": 1,
            "—Ç—Ä–µ—Ç–∏–π": 2, "—Ç—Ä–µ—Ç—å—è": 2, "—Ç—Ä–µ—Ç—å–µ": 2, "third": 2,
            "—á–µ—Ç–≤–µ—Ä—Ç—ã–π": 3, "—á–µ—Ç–≤–µ—Ä—Ç–∞—è": 3, "—á–µ—Ç–≤–µ—Ä—Ç–æ–µ": 3, "fourth": 3,
            "–ø—è—Ç—ã–π": 4, "–ø—è—Ç–∞—è": 4, "–ø—è—Ç–æ–µ": 4, "fifth": 4,
        }
        for ordinal, index in ordinal_map.items():
            if ordinal in user_response and index < len(options):
                return options[index]
        
        # Try to find by option ID
        for option in options:
            option_id = str(option.get("id", "")).lower()
            if option_id == user_response:
                return option
        
        # Try to find by label (partial match)
        user_response_lower = user_response.lower()
        for option in options:
            label = str(option.get("label", "")).lower()
            if user_response_lower in label or label in user_response_lower:
                return option
        
        # Try to find by data fields (e.g., file_name, file_id)
        for option in options:
            data = option.get("data", {})
            for key, value in data.items():
                if isinstance(value, str) and user_response_lower in value.lower():
                    return option
        
        return None

