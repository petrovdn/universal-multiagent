"""
Unified ReAct Engine - parameterized ReAct core that works with any ActionProvider.
Supports different modes (query, agent, plan) through configuration.
"""

from typing import Dict, Any, List, Optional, Literal
from dataclasses import dataclass
import asyncio
import json
import re
import time

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.tools import BaseTool

from src.core.context_manager import ConversationContext
from src.core.react_state import ReActState, ActionRecord, Observation
from src.core.result_analyzer import ResultAnalyzer, Analysis
from src.core.capability_registry import CapabilityRegistry
from src.core.action_provider import CapabilityCategory
from src.api.websocket_manager import WebSocketManager
from src.agents.model_factory import create_llm, supports_vision
from src.utils.logging_config import get_logger

logger = get_logger(__name__)


@dataclass
class ReActConfig:
    """Configuration for UnifiedReActEngine execution mode."""
    mode: Literal["query", "agent", "plan"]
    allowed_categories: List[CapabilityCategory]
    max_iterations: int = 10
    show_plan_to_user: bool = False
    require_plan_approval: bool = False
    enable_alternatives: bool = True


class UnifiedReActEngine:
    """
    Unified ReAct engine that works with CapabilityRegistry.
    Supports different modes through configuration.
    
    This engine is provider-agnostic - it doesn't know about MCP vs A2A,
    it just works with capabilities from the registry.
    """
    
    def __init__(
        self,
        config: ReActConfig,
        capability_registry: CapabilityRegistry,
        ws_manager: WebSocketManager,
        session_id: str,
        model_name: Optional[str] = None
    ):
        """
        Initialize UnifiedReActEngine.
        
        Args:
            config: ReAct configuration
            capability_registry: Capability registry with all providers
            ws_manager: WebSocket manager for events
            session_id: Session identifier
            model_name: Model name for LLM (optional)
        """
        self.config = config
        self.registry = capability_registry
        self.ws_manager = ws_manager
        self.session_id = session_id
        self.model_name = model_name
        
        # Get allowed capabilities based on config
        self.capabilities = self.registry.get_capabilities(
            categories=config.allowed_categories
        )
        
        # Build LLM tools from capabilities for planning
        self.tools = self._build_tools_from_capabilities()
        
        # Create LLM with thinking support
        self.llm = self._create_llm_with_thinking()
        
        # Bind tools to LLM
        self.llm_with_tools = self.llm.bind_tools(self.tools)
        
        # Result analyzer
        self.result_analyzer = ResultAnalyzer(model_name=model_name)
        
        # Fast LLM for simple checks (no extended thinking)
        self.fast_llm = self._create_fast_llm()
        
        # Stop flag
        self._stop_requested: bool = False
        self._current_thinking_id: Optional[str] = None  # Current thinking block ID
        self._thinking_start_time: Optional[float] = None  # Start time for elapsed calculation
        self._current_intent_id: Optional[str] = None  # Current intent block ID (Cursor-style)
        
        logger.info(
            f"[UnifiedReActEngine] Initialized for session {session_id} "
            f"with mode={config.mode}, {len(self.capabilities)} capabilities"
        )
    
    def stop(self):
        """Request stop of execution."""
        self._stop_requested = True
        logger.info(f"[UnifiedReActEngine] Stop requested for session {self.session_id}")
    
    def _build_tools_from_capabilities(self) -> List[BaseTool]:
        """
        Build LangChain tools from capabilities for LLM planning.
        
        Returns:
            List of BaseTool objects for LLM
        """
        # For now, we need to get actual BaseTool instances from MCP provider
        # This is a temporary bridge - in future, we might not need this
        tools = []
        
        # Get MCP provider if available
        for provider in self.registry.providers:
            if provider.provider_type.value == "mcp_tool":
                # MCP provider has direct access to BaseTool instances
                if hasattr(provider, 'tools'):
                    tools.extend(provider.tools.values())
                break
        
        logger.info(f"[UnifiedReActEngine] Built {len(tools)} tools for LLM planning")
        return tools
    
    def _create_fast_llm(self) -> BaseChatModel:
        """Create fast LLM for simple checks (no extended thinking)."""
        from src.utils.config_loader import get_config
        from src.agents.model_factory import create_llm
        
        config = get_config()
        # Use haiku or default model without thinking for fast responses
        try:
            return create_llm("claude-3-haiku")
        except Exception:
            return create_llm(config.default_model)
    
    def _create_llm_with_thinking(self, budget_tokens: int = 5000) -> BaseChatModel:
        """Create LLM instance with extended thinking support."""
        from src.utils.config_loader import get_config
        from langchain_anthropic import ChatAnthropic
        
        config_model_name = self.model_name or "claude-sonnet-4-5"
        config = get_config()
        
        try:
            from src.agents.model_factory import get_available_models
            available_models = get_available_models()
            
            if config_model_name in available_models:
                model_config = available_models[config_model_name]
                provider = model_config.get("provider")
                
                if provider == "anthropic" and model_config.get("supports_reasoning"):
                    reasoning_type = model_config.get("reasoning_type")
                    if reasoning_type == "extended_thinking":
                        return ChatAnthropic(
                            model=model_config["model_id"],
                            api_key=config.anthropic_api_key,
                            streaming=True,
                            temperature=1,
                            thinking={
                                "type": "enabled",
                                "budget_tokens": budget_tokens
                            }
                        )
            
            # Fallback
            return create_llm(config_model_name)
        except Exception as e:
            logger.error(f"[UnifiedReActEngine] Failed to create LLM: {e}")
            return create_llm(config.default_model)
    
    async def execute(
        self,
        goal: str,
        context: ConversationContext,
        file_ids: Optional[List[str]] = None,
        phase: Optional[str] = None  # For Plan Mode: "research", "plan", "execute"
    ) -> Dict[str, Any]:
        """
        Execute ReAct cycle for goal.
        
        Args:
            goal: User's goal
            context: Conversation context
            file_ids: Optional list of file IDs
            phase: Optional phase identifier (for Plan Mode)
            
        Returns:
            Execution result
        """
        file_ids = file_ids or []
        
        # Initialize state
        state = ReActState(goal=goal)
        state.context = {
            "file_ids": file_ids,
            "session_id": self.session_id,
            "phase": phase
        }
        self._stop_requested = False
        
        # Check if query needs tools or can be answered directly (like Cursor does)
        needs_tools = await self._needs_tools(goal, context)
        
        if not needs_tools:
            # Simple query - answer directly without tools
            logger.info(f"[UnifiedReActEngine] Simple query detected, answering directly without tools")
            try:
                return await self._answer_directly(goal, context, state)
            except Exception as e:
                logger.warning(f"[UnifiedReActEngine] Direct answer failed, falling back to ReAct: {e}")
                # Continue with normal ReAct loop if direct answer fails
        
        # Send start event (legacy)
        await self.ws_manager.send_event(
            self.session_id,
            "react_start",
            {"goal": goal, "mode": self.config.mode}
        )
        
        # Send thinking_started event (new Cursor-style)
        self._current_thinking_id = f"thinking-{int(time.time() * 1000)}"
        self._thinking_start_time = time.time()  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º—è —Å—Ç–∞—Ä—Ç–∞
        await self.ws_manager.send_event(
            self.session_id,
            "thinking_started",
            {"thinking_id": self._current_thinking_id, "started_at": int(time.time() * 1000)}
        )
        
        try:
            # Main ReAct loop
            while state.iteration < state.max_iterations:
                if self._stop_requested:
                    logger.info(f"[UnifiedReActEngine] Stop requested at iteration {state.iteration}")
                    break
                
                state.iteration += 1
                logger.info(f"[UnifiedReActEngine] Starting iteration {state.iteration}")
                
                # === EARLY INTENT: Start of iteration ===
                iteration_intent_id = f"intent-iter-{state.iteration}-{int(time.time() * 1000)}"
                files_info = ""
                if file_ids:
                    file_count = len(file_ids)
                    image_count = sum(1 for fid in file_ids if context.get_file(fid) and context.get_file(fid).get('type', '').startswith('image/'))
                    pdf_count = sum(1 for fid in file_ids if context.get_file(fid) and context.get_file(fid).get('type', '') == 'application/pdf')
                    doc_count = sum(1 for fid in file_ids if context.get_file(fid) and context.get_file(fid).get('type', '') in ('application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'application/msword'))
                    parts = []
                    if pdf_count: parts.append(f"{pdf_count} PDF")
                    if doc_count: parts.append(f"{doc_count} –¥–æ–∫—É–º–µ–Ω—Ç")
                    if image_count: parts.append(f"{image_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
                    if parts:
                        files_info = f" ({', '.join(parts)})"
                
                await self.ws_manager.send_event(
                    self.session_id,
                    "intent_start",
                    {
                        "intent_id": iteration_intent_id,
                        "text": f"–ò—Ç–µ—Ä–∞—Ü–∏—è {state.iteration}: –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∑–∞–ø—Ä–æ—Å{files_info}..."
                    }
                )
                
                # 1. THINK - Analyze current situation
                state.status = "thinking"
                # NOTE: Removed static message - progress updates will show progress
                
                # Start progress updates while LLM thinks
                think_progress_messages = [
                    "–ò–∑—É—á–∞—é –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞...",
                    "–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–¥–∞—á–∏...",
                    "–ò–∑–≤–ª–µ–∫–∞—é –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é...",
                    "–û–ø—Ä–µ–¥–µ–ª—è—é —Ç—Ä–µ–±—É–µ–º—ã–µ –¥–µ–π—Å—Ç–≤–∏—è...",
                ]
                think_progress_task = asyncio.create_task(
                    self._send_progress_updates(iteration_intent_id, think_progress_messages, interval=5.0)
                )
                
                try:
                    thought = await self._think(state, context, file_ids)
                finally:
                    think_progress_task.cancel()
                    try:
                        await think_progress_task
                    except asyncio.CancelledError:
                        pass
                
                state.current_thought = thought
                state.add_reasoning_step("think", thought)
                await self._stream_reasoning("react_thinking", {
                    "thought": thought,
                    "iteration": state.iteration
                })
                
                if self._stop_requested:
                    break
                
                # 2. PLAN - Choose next action
                state.status = "acting"
                # NOTE: Removed static message - progress updates will show progress
                
                # Start progress updates while LLM plans
                plan_progress_messages = [
                    "–û—Ü–µ–Ω–∏–≤–∞—é –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã...",
                    "–í—ã–±–∏—Ä–∞—é –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é...",
                    "–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—é –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ–π—Å—Ç–≤–∏—è...",
                ]
                plan_progress_task = asyncio.create_task(
                    self._send_progress_updates(iteration_intent_id, plan_progress_messages, interval=5.0)
                )
                
                try:
                    action_plan = await self._plan_action(state, thought, context, file_ids)
                finally:
                    plan_progress_task.cancel()
                    try:
                        await plan_progress_task
                    except asyncio.CancelledError:
                        pass
                
                # Complete iteration intent
                await self.ws_manager.send_event(
                    self.session_id,
                    "intent_complete",
                    {"intent_id": iteration_intent_id, "summary": f"–ò—Ç–µ—Ä–∞—Ü–∏—è {state.iteration} –∑–∞–≤–µ—Ä—à–µ–Ω–∞"}
                )
                
                # Check for special "FINISH" marker
                tool_name = action_plan.get("tool_name", "")
                if tool_name.upper() == "FINISH" or tool_name == "finish":
                    logger.info(f"[UnifiedReActEngine] LLM indicated task completion")
                    finish_reasoning = action_plan.get("reasoning", "–ó–∞–¥–∞—á–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
                    finish_description = action_plan.get("description", "–ó–∞–¥–∞—á–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
                    state.add_reasoning_step("plan", finish_reasoning, {
                        "tool": "FINISH",
                        "marker": True
                    })
                    await self._stream_reasoning("react_action", {
                        "action": finish_description,
                        "tool": "FINISH",
                        "params": {},
                        "iteration": state.iteration
                    })
                    # Add a synthetic observation with the reasoning for final answer generation
                    finish_action = state.add_action("FINISH", {})
                    state.add_observation(
                        action=finish_action,
                        raw_result=finish_reasoning,
                        success=True
                    )
                    return await self._finalize_success(
                        state,
                        finish_description,
                        context,
                        file_ids
                    )
                
                state.add_reasoning_step("plan", action_plan.get("reasoning", ""), {
                    "tool": action_plan.get("tool_name"),
                    "arguments": action_plan.get("arguments", {})
                })
                await self._stream_reasoning("react_action", {
                    "action": action_plan.get("description", ""),
                    "tool": action_plan.get("tool_name"),
                    "params": action_plan.get("arguments", {}),
                    "iteration": state.iteration
                })
                
                if self._stop_requested:
                    break
                
                # 3. ACT - Execute action through registry
                action_record = state.add_action(
                    action_plan.get("tool_name", "unknown"),
                    action_plan.get("arguments", {})
                )
                
                try:
                    result = await self._execute_action(action_plan, context)
                except Exception as e:
                    logger.error(f"[UnifiedReActEngine] Action execution failed: {e}")
                    result = f"Error: {str(e)}"
                
                # 4. OBSERVE - Analyze result
                state.status = "observing"
                observation = state.add_observation(
                    action_record,
                    result,
                    success=True  # Will be updated by analyzer
                )
                
                await self._stream_reasoning("react_observation", {
                    "result": str(result),  # Full result - no truncation
                    "iteration": state.iteration
                })
                
                # Analyze result
                analysis = await self.result_analyzer.analyze(
                    action_record,
                    result,
                    state.goal,
                    state.observations[:-1]
                )
                
                # Update observation with analysis
                observation.success = analysis.is_success
                observation.error_message = analysis.error_message
                observation.extracted_data = analysis.extracted_data
                
                state.add_reasoning_step("observe", f"Analysis: {analysis.progress_toward_goal:.0%} progress", {
                    "success": analysis.is_success,
                    "progress": analysis.progress_toward_goal,
                    "error": analysis.error_message
                })
                
                # 5. ADAPT - Make decision
                state.status = "adapting"
                
                if analysis.is_goal_achieved:
                    logger.info(f"[UnifiedReActEngine] Goal achieved at iteration {state.iteration}")
                    return await self._finalize_success(state, result, context, file_ids)
                
                elif analysis.is_error:
                    if self.config.enable_alternatives:
                        alternative = await self._find_alternative(state, analysis, context, file_ids)
                        if alternative:
                            logger.info(f"[UnifiedReActEngine] Trying alternative: {alternative.get('description', '')}")
                            state.alternatives_tried.append(alternative.get("description", ""))
                            state.add_reasoning_step("adapt", f"Trying alternative: {alternative.get('description', '')}", {
                                "alternative": alternative
                            })
                            await self._stream_reasoning("react_adapting", {
                                "reason": analysis.error_message or "Action failed",
                                "new_strategy": alternative.get("description", ""),
                                "iteration": state.iteration
                            })
                            # Continue loop with alternative
                        else:
                            logger.warning(f"[UnifiedReActEngine] No alternatives found, failing gracefully")
                            return await self._finalize_failure(state, analysis, context)
                    else:
                        return await self._finalize_failure(state, analysis, context)
                else:
                    # Progress made, continue
                    state.add_reasoning_step("adapt", "Continuing with progress", {
                        "progress": analysis.progress_toward_goal
                    })
                    logger.info(f"[UnifiedReActEngine] Progress: {analysis.progress_toward_goal:.0%}")
            
            # Max iterations reached
            logger.warning(f"[UnifiedReActEngine] Max iterations reached")
            return await self._finalize_timeout(state, context)
            
        except Exception as e:
            logger.error(f"[UnifiedReActEngine] Error in execute: {e}", exc_info=True)
            await self.ws_manager.send_event(
                self.session_id,
                "react_failed",
                {
                    "reason": str(e),
                    "tried": [alt for alt in state.alternatives_tried]
                }
            )
            raise
    
    async def _needs_tools(self, goal: str, context: ConversationContext) -> bool:
        """
        Determine if the query needs tools or can be answered directly.
        
        Simple queries (greetings, simple questions) don't need tools.
        Complex queries (data retrieval, file operations) need tools.
        Also checks conversation context for follow-up queries.
        """
        goal_lower = goal.lower().strip()
        
        # Simple greetings and basic questions - no tools needed
        simple_patterns = [
            r'^(–ø—Ä–∏–≤–µ—Ç|hello|hi|–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π|–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ|–¥–æ–±—Ä—ã–π\s+(–¥–µ–Ω—å|–≤–µ—á–µ—Ä|—É—Ç—Ä–æ))',
            r'^(—Å–ø–∞—Å–∏–±–æ|thanks|thank\s+you|–±–ª–∞–≥–æ–¥–∞—Ä—é)',
            r'^(–∫–∞–∫\s+–¥–µ–ª–∞|how\s+are\s+you|—á—Ç–æ\s+—Ç—ã|who\s+are\s+you|—á—Ç–æ\s+—É–º–µ–µ—à—å)',
            r'^(–ø–æ–∫–∞|bye|goodbye|–¥–æ\s+—Å–≤–∏–¥–∞–Ω–∏—è)',
        ]
        
        for pattern in simple_patterns:
            if re.match(pattern, goal_lower):
                return False
        
        # Check for simple generative patterns (poems, jokes, greetings, etc.) - no tools needed
        simple_generative_patterns = [
            r"(–Ω–∞–ø–∏—à–∏|—Å–æ—Å—Ç–∞–≤—å|—Å–æ—á–∏–Ω–∏|–ø—Ä–∏–¥—É–º–∞–π|—Å–æ–∑–¥–∞–π)\s+(–º–Ω–µ\s+)?(–∫—Ä–∞—Ç–∫–æ–µ\s+)?(–ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ|—Å—Ç–∏—Ö|—Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ|—à—É—Ç–∫—É|–∞–Ω–µ–∫–¥–æ—Ç|—Å–æ–æ–±—â–µ–Ω–∏–µ|—Ç–µ–∫—Å—Ç|–ø–∏—Å—å–º–æ|—Ö–æ–∫–∫—É|—Ö–∞–π–∫—É|haiku|—Ä–∞—Å—Å–∫–∞–∑|–∏—Å—Ç–æ—Ä–∏—é|—Å–∫–∞–∑–∫—É|–ø–µ—Å–Ω—é)",
            r"(–Ω–∞–ø–∏—à–∏|—Å–æ—Å—Ç–∞–≤—å|—Å–æ—á–∏–Ω–∏|–ø—Ä–∏–¥—É–º–∞–π)\s+\w*\s*(—Ö–æ–∫–∫—É|—Ö–∞–π–∫—É|haiku)",
            r"write\s+(me\s+)?(a\s+)?(greeting|poem|joke|message|story|haiku)",
            # Direct creative requests
            r"^(—Ö–æ–∫–∫—É|—Ö–∞–π–∫—É|haiku|—Å—Ç–∏—Ö|–∞–Ω–µ–∫–¥–æ—Ç|—à—É—Ç–∫–∞)$",
            r"^(–Ω–∞–ø–∏—à–∏|—Å–æ—Å—Ç–∞–≤—å|—Å–æ—á–∏–Ω–∏|–ø—Ä–∏–¥—É–º–∞–π)\s+\w{2,}$",  # "–Ω–∞–ø–∏—à–∏ —Ö–æ–∫–∫—É", "—Å–æ—Å—Ç–∞–≤—å —Ä–∞—Å—Å–∫–∞–∑"
        ]
        
        for pattern in simple_generative_patterns:
            if re.search(pattern, goal_lower):
                return False
        
        # Check if query mentions specific actions that require tools
        tool_keywords = [
            '–Ω–∞–π–¥–∏', 'find', '–ø–æ–ª—É—á–∏', 'get', '–≤—ã–≤–µ–¥–∏', 'show', '–æ—Ç–∫—Ä–æ–π', 'open',
            '—Å–æ–∑–¥–∞–π', 'create', '–æ—Ç–ø—Ä–∞–≤—å', 'send', '—Å–æ—Ö—Ä–∞–Ω–∏', 'save',
            '–∫–∞–ª–µ–Ω–¥–∞—Ä—å', 'calendar', 
            # Russian word forms for "–≤—Å—Ç—Ä–µ—á–∞" (meeting) - all cases
            '–≤—Å—Ç—Ä–µ—á–∏', '–≤—Å—Ç—Ä–µ—á', '–≤—Å—Ç—Ä–µ—á–∞', '–≤—Å—Ç—Ä–µ—á—É', '–≤—Å—Ç—Ä–µ—á–µ–π', '–≤—Å—Ç—Ä–µ—á–∞–º', '–≤—Å—Ç—Ä–µ—á–∞–º–∏', '–≤—Å—Ç—Ä–µ—á–∞—Ö',
            'events', 'meetings', 'event', 'meeting',
            '–ø–∏—Å—å–º–∞', 'emails', '–ø–æ—á—Ç–∞', 'mail',
            '—Ç–∞–±–ª–∏—Ü–∞', 'table', 'sheets', '–¥–æ–∫—É–º–µ–Ω—Ç', 'document',
            '—Ñ–∞–π–ª', 'file', '–¥–∞–Ω–Ω—ã–µ', 'data',
            # 1C / Accounting keywords
            '–ø—Ä–æ–≤–æ–¥–∫', '1—Å', '1c', '–±—É—Ö–≥–∞–ª—Ç–µ—Ä', '–≤—ã—Ä—É—á–∫', '–æ—Å—Ç–∞—Ç–∫', '—Å–∫–ª–∞–¥',
            # Project Lad keywords
            '–ø—Ä–æ–µ–∫—Ç', '–ø–æ—Ä—Ç—Ñ–µ–ª', '–≥–∞–Ω—Ç', '–≤–µ—Ö', '—Ä–∞–±–æ—Ç', 'project lad', 'projectlad'
        ]
        
        # Check for specific calendar-related patterns
        calendar_patterns = [
            r'—Å–ø–∏—Å–æ–∫\s+–≤—Å—Ç—Ä–µ—á',  # "—Å–ø–∏—Å–æ–∫ –≤—Å—Ç—Ä–µ—á" (list of meetings)
            r'–≤—Å—Ç—Ä–µ—á[–∞–∏]?\s+–Ω–∞\s+(—ç—Ç–æ–π|—Å–ª–µ–¥—É—é—â–µ–π|–ø—Ä–æ—à–ª–æ–π)\s+–Ω–µ–¥–µ–ª–µ',  # "–≤—Å—Ç—Ä–µ—á–∏ –Ω–∞ —ç—Ç–æ–π –Ω–µ–¥–µ–ª–µ"
            r'–≤—Å—Ç—Ä–µ—á[–∞–∏]?\s+(—Å–µ–≥–æ–¥–Ω—è|–∑–∞–≤—Ç—Ä–∞|–ø–æ—Å–ª–µ–∑–∞–≤—Ç—Ä–∞)',  # "–≤—Å—Ç—Ä–µ—á–∏ —Å–µ–≥–æ–¥–Ω—è"
            r'—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ\s+(–Ω–∞|–Ω–∞\s+—ç—Ç–æ–π)',  # "—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —ç—Ç–æ–π –Ω–µ–¥–µ–ª–µ"
        ]
        
        for pattern in calendar_patterns:
            if re.search(pattern, goal_lower):
                return True
        
        for keyword in tool_keywords:
            if keyword in goal_lower:
                return True
        
        # === NEW: Check for follow-up/clarification queries that reference previous context ===
        # These patterns indicate user is asking for more info about a previous topic
        followup_patterns = [
            r'^–∞\s+(–Ω–∞|–≤|–∑–∞|—á—Ç–æ|–∫–∞–∫|–≥–¥–µ|–∫–æ–≥–¥–∞|—Å–∫–æ–ª—å–∫–æ)',  # "–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π –Ω–µ–¥–µ–ª–µ?", "–∞ –≤ –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫?"
            r'^(–∞|–∏|–µ—â–µ|–µ—â—ë|—Ç–∞–∫–∂–µ|—Ç–æ–∂–µ)\s',  # "–∞ ...", "–µ—â–µ –ø–æ–∫–∞–∂–∏", "—Ç–∞–∫–∂–µ ..."
            r'^(–Ω–∞|–≤|–∑–∞)\s+(—Å–ª–µ–¥—É—é—â|–ø—Ä–æ—à–ª|—ç—Ç)',  # "–Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π –Ω–µ–¥–µ–ª–µ", "–≤ –ø—Ä–æ—à–ª—ã–π —Ä–∞–∑"
            r'(—Å–ª–µ–¥—É—é—â|–ø—Ä–æ—à–ª|–ø—Ä–µ–¥—ã–¥—É—â)\s*(–Ω–µ–¥–µ–ª|–º–µ—Å—è—Ü|–¥–µ–Ω—å|–≥–æ–¥)',  # "—Å–ª–µ–¥—É—é—â–µ–π –Ω–µ–¥–µ–ª–µ", "–ø—Ä–æ—à–ª–æ–º –º–µ—Å—è—Ü–µ"
            r'^(—á—Ç–æ|–∫–∞–∫–∏–µ|—Å–∫–æ–ª—å–∫–æ)\s+(—Ç–∞–º|–µ—â–µ|–µ—â—ë)',  # "—á—Ç–æ —Ç–∞–º –µ—â–µ?"
            r'^(–ø–æ–∫–∞–∂–∏|–≤—ã–≤–µ–¥–∏|–¥–∞–π)\s+(–µ—â–µ|–µ—â—ë|–±–æ–ª—å—à–µ|–¥—Ä—É–≥–∏–µ)',  # "–ø–æ–∫–∞–∂–∏ –µ—â–µ", "–¥–∞–π –±–æ–ª—å—à–µ"
        ]
        
        is_followup = any(re.search(pattern, goal_lower) for pattern in followup_patterns)
        
        # If it looks like a follow-up, check previous context for tool-related topics
        if is_followup and hasattr(context, 'messages') and context.messages:
            recent_messages = context.get_recent_messages(6)  # Last 3 exchanges
            
            # Context keyword groups for different tool categories
            context_keyword_groups = {
                'calendar': ['–≤—Å—Ç—Ä–µ—á', '–∫–∞–ª–µ–Ω–¥–∞—Ä', '—Å–æ–±—ã—Ç–∏', '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏', 'meeting', 'event', 'calendar', 'schedule'],
                'email': ['–ø–∏—Å—å–º', '–ø–æ—á—Ç', 'email', 'mail', '—Å–æ–æ–±—â–µ–Ω–∏'],
                'files': ['—Ñ–∞–π–ª', '–¥–æ–∫—É–º–µ–Ω—Ç', 'file', 'document'],
                'sheets': ['—Ç–∞–±–ª–∏—Ü', 'sheet', 'spreadsheet', '—è—á–µ–π–∫', '—Å—Ç–æ–ª–±—Ü', '—Å—Ç—Ä–æ–∫'],
                'accounting': ['–ø—Ä–æ–≤–æ–¥–∫', '1—Å', '1c', '–±—É—Ö–≥–∞–ª—Ç–µ—Ä', '–≤—ã—Ä—É—á–∫', '–æ—Å—Ç–∞—Ç–∫', '—Å–∫–ª–∞–¥', '—É—á–µ—Ç', '—É—á—ë—Ç', 'odata'],
                'projectlad': ['–ø—Ä–æ–µ–∫—Ç', '–ø–æ—Ä—Ç—Ñ–µ–ª', '–≥–∞–Ω—Ç', '–≤–µ—Ö', '—Ä–∞–±–æ—Ç', 'project lad', 'projectlad', 'pl', '–ø–ª', '–¥–∏–∞–≥—Ä–∞–º–º']
            }
            
            # Check recent messages for context
            for msg in recent_messages:
                msg_content = msg.get('content', '').lower()
                
                for category, keywords in context_keyword_groups.items():
                    if any(kw in msg_content for kw in keywords):
                        logger.info(f"[UnifiedReActEngine] Follow-up detected with {category} context")
                        return True
        
        # Use LLM to determine if tools are needed (for edge cases)
        # NOW with context!
        try:
            # Build context string from recent messages
            context_str = ""
            if hasattr(context, 'messages') and context.messages:
                recent = context.get_recent_messages(4)
                if recent:
                    context_str = "\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π:\n"
                    for msg in recent:
                        role = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å" if msg.get('role') == 'user' else "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç"
                        content = msg.get('content', '')[:200]  # Truncate
                        context_str += f"{role}: {content}\n"
            
            prompt = f"""–û–ø—Ä–µ–¥–µ–ª–∏, –Ω—É–∂–Ω—ã –ª–∏ –í–ù–ï–®–ù–ò–ï –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –∑–∞–ø—Ä–æ—Å:

–ó–∞–ø—Ä–æ—Å: "{goal}"
{context_str}

–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: –î–ê –∏–ª–∏ –ù–ï–¢.

–ù–ï–¢ - –µ—Å–ª–∏ —ç—Ç–æ:
- –ü—Ä–æ—Å—Ç–æ–π –≤–æ–ø—Ä–æ—Å, –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ, –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å
- –¢–í–û–†–ß–ï–°–ö–ê–Ø –ø—Ä–æ—Å—å–±–∞: –Ω–∞–ø–∏—Å–∞—Ç—å —Å—Ç–∏—Ö, —Ö–æ–∫–∫—É, —Ä–∞—Å—Å–∫–∞–∑, —à—É—Ç–∫—É, –∏—Å—Ç–æ—Ä–∏—é, —Å–æ—á–∏–Ω–∏—Ç—å —Ç–µ–∫—Å—Ç
- –õ—é–±–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –∑–∞–¥–∞—á–∞, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ë–ï–ó –≤–Ω–µ—à–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö

–î–ê - –µ—Å–ª–∏ –Ω—É–∂–Ω—ã –í–ù–ï–®–ù–ò–ï –¥–∞–Ω–Ω—ã–µ –∏–∑:
- –ö–∞–ª–µ–Ω–¥–∞—Ä—å: "–Ω–∞–π–¥–∏ –≤—Å—Ç—Ä–µ—á–∏", "–ø–æ–∫–∞–∂–∏ —Å–æ–±—ã—Ç–∏—è –Ω–∞ –Ω–µ–¥–µ–ª–µ"
- –ü–æ—á—Ç–∞: "–ø–æ–∫–∞–∂–∏ –ø–∏—Å—å–º–∞", "–Ω–µ–ø—Ä–æ—á–∏—Ç–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è"  
- –§–∞–π–ª—ã: "–æ—Ç–∫—Ä–æ–π —Ñ–∞–π–ª", "–Ω–∞–π–¥–∏ –¥–æ–∫—É–º–µ–Ω—Ç"
- –¢–∞–±–ª–∏—Ü—ã: "–¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã", "–∑–Ω–∞—á–µ–Ω–∏—è –≤ —è—á–µ–π–∫–∞—Ö"
- 1–°/–ë—É—Ö–≥–∞–ª—Ç–µ—Ä–∏—è: "–ø—Ä–æ–≤–æ–¥–∫–∏", "–æ—Å—Ç–∞—Ç–∫–∏ –Ω–∞ —Å–∫–ª–∞–¥–∞—Ö", "–≤—ã—Ä—É—á–∫–∞"
- Project Lad: "–ø—Ä–æ–µ–∫—Ç—ã", "–ø–æ—Ä—Ç—Ñ–µ–ª—å", "–¥–∏–∞–≥—Ä–∞–º–º–∞ –≥–∞–Ω—Ç–∞", "–≤–µ—Ö–∏"

–í–ê–ñ–ù–û: 
- –ï—Å–ª–∏ —ç—Ç–æ –£–¢–û–ß–ù–Ø–Æ–©–ò–ô –≤–æ–ø—Ä–æ—Å (–Ω–∞–ø—Ä–∏–º–µ—Ä "–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π –Ω–µ–¥–µ–ª–µ?", "–∞ –∑–∞ –ø—Ä–æ—à–ª—ã–π –º–µ—Å—è—Ü?", "–µ—â–µ –ø–æ–∫–∞–∂–∏") 
  –∏ –≤ –ö–û–ù–¢–ï–ö–°–¢–ï –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å –≤—Å—Ç—Ä–µ—á–∏/–ø–∏—Å—å–º–∞/—Ñ–∞–π–ª—ã/—Ç–∞–±–ª–∏—Ü—ã/–ø—Ä–æ–≤–æ–¥–∫–∏/–ø—Ä–æ–µ–∫—Ç—ã - —ç—Ç–æ –î–ê, –Ω—É–∂–Ω—ã —Ç–µ –∂–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã.
- –ö–æ—Ä–æ—Ç–∫–∏–µ —É—Ç–æ—á–Ω–µ–Ω–∏—è —Ç–∏–ø–∞ "–∞ –≤—á–µ—Ä–∞?", "–∞ —Ç–∞–º?" –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Ç–µ–º–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞."""
            
            messages = [
                SystemMessage(content="–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –£—á–∏—Ç—ã–≤–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –î–ê –∏–ª–∏ –ù–ï–¢."),
                HumanMessage(content=prompt)
            ]
            
            # Use fast LLM (no extended thinking) for quick classification
            response = await self.fast_llm.ainvoke(messages)
            response_text = str(response.content).strip().upper()
            
            llm_result = "–î–ê" in response_text or "YES" in response_text
            return llm_result
        except Exception as e:
            logger.error(f"[UnifiedReActEngine] Error checking if tools needed: {e}")
            # Default to using tools if check fails
            return True
    
    async def _answer_directly(
        self,
        goal: str,
        context: ConversationContext,
        state: ReActState
    ) -> Dict[str, Any]:
        """
        Answer simple queries directly without using tools.
        This mimics Cursor's behavior for simple queries.
        Properly passes conversation history for reference resolution.
        """
        try:
            # Check if model uses extended thinking
            uses_extended_thinking = False
            try:
                from src.agents.model_factory import get_available_models
                available_models = get_available_models()
                if self.model_name and self.model_name in available_models:
                    model_config = available_models[self.model_name]
                    if model_config.get("reasoning_type") == "extended_thinking":
                        uses_extended_thinking = True
            except:
                pass
            
            # Build messages list with proper conversation history
            messages = [
                SystemMessage(content="""–¢—ã –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. 
–û—Ç–≤–µ—á–∞–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –∏ –∫—Ä–∞—Ç–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
–£—á–∏—Ç—ã–≤–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–µ.
–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ —á—Ç–æ-—Ç–æ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä "–ø–µ—Ä–µ–¥–µ–ª–∞–π –µ–≥–æ", "—Å–¥–µ–ª–∞–π –µ—â–µ"), –∏—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞.""")
            ]
            
            # Add conversation history as proper messages (for reference resolution)
            if hasattr(context, 'messages') and context.messages:
                recent_messages = context.messages[-6:]  # Last 6 messages (3 exchanges)
                for msg in recent_messages:
                    role = msg.get('role', 'user')
                    content = msg.get('content', '')
                    if not content:
                        continue
                    
                    if role == 'user':
                        messages.append(HumanMessage(content=content))
                    elif role == 'assistant':
                        # For extended thinking models, wrap as HumanMessage to avoid API errors
                        if uses_extended_thinking:
                            messages.append(HumanMessage(
                                content=f"[–ü—Ä–µ–¥—ã–¥—É—â–∏–π –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞]:\n{content}"
                            ))
                        else:
                            messages.append(AIMessage(content=content))
            
            # Add current user request
            messages.append(HumanMessage(content=goal))
            
            # Send thinking_started event
            self._current_thinking_id = f"thinking-{int(time.time() * 1000)}"
            self._thinking_start_time = time.time()
            await self.ws_manager.send_event(
                self.session_id,
                "thinking_started",
                {"thinking_id": self._current_thinking_id, "started_at": int(time.time() * 1000)}
            )
            
            response = await self.llm.ainvoke(messages)
            
            # Extract response text
            if isinstance(response.content, list):
                text_parts = []
                for block in response.content:
                    if hasattr(block, "text"):
                        text_parts.append(block.text)
                    elif isinstance(block, dict) and "text" in block:
                        text_parts.append(block["text"])
                    elif isinstance(block, str):
                        text_parts.append(block)
                answer = " ".join(text_parts).strip()
            elif isinstance(response.content, str):
                answer = response.content.strip()
            else:
                answer = str(response.content).strip()
            
            # Send thinking_completed
            if self._current_thinking_id:
                elapsed_seconds = time.time() - self._thinking_start_time
                await self.ws_manager.send_event(
                    self.session_id,
                    "thinking_completed",
                    {
                        "thinking_id": self._current_thinking_id,
                        "full_content": answer,
                        "elapsed_seconds": elapsed_seconds,
                        "auto_collapse": True
                    }
                )
                self._current_thinking_id = None
                self._thinking_start_time = None
            
            # Send final result or message_complete based on mode
            if self.config.mode == "query":
                await self.ws_manager.send_event(
                    self.session_id,
                    "final_result",
                    {"content": answer}
                )
            else:
                message_id = f"react_{self.session_id}_{int(time.time() * 1000)}"
                await self.ws_manager.send_event(
                    self.session_id,
                    "message_complete",
                    {
                        "role": "assistant",
                        "message_id": message_id,
                        "content": answer
                    }
                )
            
            return {
                "status": "completed",
                "goal": goal,
                "iterations": 1,
                "actions_taken": 0,
                "final_result": answer,
                "reasoning_trail": [
                    {
                        "iteration": 1,
                        "type": "direct_answer",
                        "content": answer,
                        "metadata": {"simple_query": True}
                    }
                ]
            }
        except Exception as e:
            logger.error(f"[UnifiedReActEngine] Error in _answer_directly: {e}")
            # If direct answer fails, raise exception to fall back to normal ReAct loop
            raise
    
    async def _send_progress_updates(
        self,
        intent_id: str,
        messages: List[str],
        interval: float = 5.0
    ) -> None:
        """
        Send progress updates every interval seconds until cancelled.
        
        This runs as a background task to show user that work is happening
        during long LLM operations.
        """
        try:
            for msg in messages:
                await asyncio.sleep(interval)
                await self.ws_manager.send_event(
                    self.session_id,
                    "intent_detail",
                    {"intent_id": intent_id, "type": "analyze", "description": msg}
                )
        except asyncio.CancelledError:
            # Task was cancelled, this is expected
            pass
    
    async def _think(
        self,
        state: ReActState,
        context: ConversationContext,
        file_ids: List[str]
    ) -> str:
        """Generate thought about current situation."""
        context_str = f"–¶–µ–ª—å: {state.goal}\n\n"
        
        # Add conversation history for reference resolution (NEW)
        if hasattr(context, 'messages') and context.messages:
            recent_messages = context.messages[-4:]  # Last 2 exchanges
            if recent_messages:
                context_str += "üìù –ö–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ (–¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤):\n"
                for msg in recent_messages:
                    role = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å" if msg.get('role') == 'user' else "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç"
                    content = msg.get('content', '')[:300]  # Truncate
                    context_str += f"  {role}: {content}\n"
                context_str += "\n"
        
        # Add file context (uploaded files have PRIORITY #1)
        if file_ids:
            uploaded_files_found = []
            for file_id in file_ids:
                file_data = context.get_file(file_id)
                if file_data:
                    uploaded_files_found.append(file_data)
            if uploaded_files_found:
                context_str += "üìé –ü—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:\n"
                for file_data in uploaded_files_found:
                    filename = file_data.get('filename', 'unknown')
                    file_type = file_data.get('type', '')
                    if file_type == 'application/pdf' and 'text' in file_data:
                        pdf_text = file_data.get('text', '')
                        max_len = 8000  # Increased for better analysis
                        if len(pdf_text) > max_len:
                            pdf_text = pdf_text[:max_len] + "\n... (–æ–±—Ä–µ–∑–∞–Ω–æ, –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç " + str(len(file_data.get('text', ''))) + " —Å–∏–º–≤–æ–ª–æ–≤)"
                        context_str += f"- PDF: {filename}\n{pdf_text}\n"
                    elif file_type in ("application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                      "application/msword") and 'text' in file_data:
                        docx_text = file_data.get('text', '')
                        max_len = 8000  # Increased for better analysis
                        if len(docx_text) > max_len:
                            docx_text = docx_text[:max_len] + "\n... (–æ–±—Ä–µ–∑–∞–Ω–æ, –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç " + str(len(file_data.get('text', ''))) + " —Å–∏–º–≤–æ–ª–æ–≤)"
                        context_str += f"- Word –¥–æ–∫—É–º–µ–Ω—Ç: {filename}\n{docx_text}\n"
                    else:
                        context_str += f"- {filename}\n"
        
        # Add open files context (PRIORITY #2)
        open_files = context.get_open_files() if hasattr(context, 'get_open_files') else []
        if open_files:
            context_str += "üìÇ –û—Ç–∫—Ä—ã—Ç—ã–µ —Ñ–∞–π–ª—ã –≤ —Ä–∞–±–æ—á–µ–π –æ–±–ª–∞—Å—Ç–∏:\n"
            for file in open_files:
                title = file.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
                context_str += f"- {title}\n"
        
        if state.action_history:
            context_str += "\n–í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è:\n"
            for i, action in enumerate(state.action_history[-5:], 1):
                obs = next((o for o in state.observations if o.action == action), None)
                status = "‚úì" if obs and obs.success else "‚úó"
                context_str += f"{i}. {status} {action.tool_name}\n"
        
        if state.observations:
            context_str += "\n–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n"
            for obs in state.observations[-3:]:
                result_preview = str(obs.raw_result)[:200]
                context_str += f"- {obs.action.tool_name}: {result_preview}...\n"
        
        prompt = f"""–¢—ã –≤—ã–ø–æ–ª–Ω—è–µ—à—å –∑–∞–¥–∞—á—É –ø–æ—à–∞–≥–æ–≤–æ, –∏—Å–ø–æ–ª—å–∑—É—è –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã.

{context_str}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—É—â—É—é —Å–∏—Ç—É–∞—Ü–∏—é:
1. –ß—Ç–æ —É–∂–µ —Å–¥–µ–ª–∞–Ω–æ?
2. –ß—Ç–æ –æ—Å—Ç–∞–ª–æ—Å—å —Å–¥–µ–ª–∞—Ç—å –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏?
3. –ö–∞–∫–æ–µ —Å–ª–µ–¥—É—é—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ –±—É–¥–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º?

–î–∞–π –∫—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."""

        try:
            messages = [
                SystemMessage(content="–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –∑–∞–¥–∞—á –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –¥–µ–π—Å—Ç–≤–∏–π. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."),
                HumanMessage(content=prompt)
            ]
            
            # Stream thinking process
            thought = ""
            thinking_id = f"thinking_{self.session_id}_{int(time.time() * 1000)}"
            
            # Send thinking start
            await self.ws_manager.send_event(
                self.session_id,
                "thinking_started",
                {"thinking_id": thinking_id}
            )
            
            async for chunk in self.llm.astream(messages):
                chunk_text = ""
                if hasattr(chunk, 'content') and chunk.content:
                    if isinstance(chunk.content, list):
                        for block in chunk.content:
                            if hasattr(block, "text"):
                                chunk_text += block.text
                            elif isinstance(block, dict) and "text" in block:
                                chunk_text += block["text"]
                            elif isinstance(block, str):
                                chunk_text += block
                    elif isinstance(chunk.content, str):
                        chunk_text = chunk.content
                elif isinstance(chunk, str):
                    chunk_text = chunk
                
                if chunk_text:
                    thought += chunk_text
                    await self.ws_manager.send_event(
                        self.session_id,
                        "thinking_chunk",
                        {
                            "thinking_id": thinking_id,
                            "chunk": chunk_text  # Frontend expects "chunk" not "content"
                        }
                    )
            
            # Complete thinking
            await self.ws_manager.send_event(
                self.session_id,
                "thinking_completed",
                {"thinking_id": thinking_id}
            )
            
            return thought.strip()
        except Exception as e:
            logger.error(f"[UnifiedReActEngine] Error in _think: {e}")
            return f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å–∏—Ç—É–∞—Ü–∏—é... (–∏—Ç–µ—Ä–∞—Ü–∏—è {state.iteration})"
    
    async def _plan_action(
        self,
        state: ReActState,
        thought: str,
        context: ConversationContext,
        file_ids: List[str]
    ) -> Dict[str, Any]:
        """Plan next action based on thought."""
        # Get capability descriptions (filtered by allowed categories)
        capability_descriptions = []
        for cap in self.capabilities[:50]:  # Limit to first 50
            capability_descriptions.append(f"- {cap.name}: {cap.description}")
        
        tools_str = "\n".join(capability_descriptions)
        
        # Build context
        context_str = f"–¶–µ–ª—å: {state.goal}\n\n"
        context_str += f"–¢–µ–∫—É—â–∏–π –∞–Ω–∞–ª–∏–∑: {thought}\n\n"
        
        # Add conversation history for reference resolution (NEW)
        if hasattr(context, 'messages') and context.messages:
            recent_messages = context.messages[-4:]  # Last 2 exchanges
            if recent_messages:
                context_str += "üìù –ö–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ (–¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤ —Ç–∏–ø–∞ '–µ–≥–æ', '—ç—Ç–æ', '–µ—â–µ'):\n"
                for msg in recent_messages:
                    role = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å" if msg.get('role') == 'user' else "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç"
                    content = msg.get('content', '')[:300]  # Truncate
                    context_str += f"  {role}: {content}\n"
                context_str += "\n"
        
        if state.action_history:
            context_str += "–£–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ:\n"
            for action in state.action_history[-3:]:
                context_str += f"- {action.tool_name}\n"
        
        # Add uploaded files context (PRIORITY #1) - must come FIRST
        if file_ids:
            uploaded_files_found = []
            for file_id in file_ids:
                file_data = context.get_file(file_id)
                if file_data:
                    uploaded_files_found.append(file_data)
            
            if uploaded_files_found:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É vision —É –º–æ–¥–µ–ª–∏
                model_supports_vision = supports_vision(self.model_name) if self.model_name else False
                
                context_str += "\nüìé –ü–†–ò–ö–†–ï–ü–õ–ï–ù–ù–´–ï –§–ê–ô–õ–´ (–ü–†–ò–û–†–ò–¢–ï–¢ #1 - –∏—Å–ø–æ–ª—å–∑—É–π –∏—Ö –ü–ï–†–í–´–ú!):\n"
                has_images = False
                for file_data in uploaded_files_found:
                    filename = file_data.get('filename', 'unknown')
                    file_type = file_data.get('type', '')
                    if file_type.startswith('image/'):
                        has_images = True
                        if model_supports_vision:
                            context_str += f"- –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {filename} (–£–ñ–ï –ü–ï–†–ï–î–ê–ù–û –í –≠–¢–û–ú –°–û–û–ë–©–ï–ù–ò–ò —á–µ—Ä–µ–∑ Vision API - –≤–∏–¥–∏—à—å –µ–≥–æ –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å!)\n"
                        else:
                            context_str += f"- –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {filename} (–º–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç vision, –ø—Ä–æ–ø—É—â–µ–Ω–æ)\n"
                            logger.warning(f"Model {self.model_name} doesn't support vision, skipping image {filename}")
                    elif file_type == 'application/pdf' and 'text' in file_data:
                        pdf_text = file_data.get('text', '')
                        # Truncate if too long - increased limit for better analysis
                        max_len = 10000
                        if len(pdf_text) > max_len:
                            pdf_text = pdf_text[:max_len] + "\n... (—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω, –ø–æ–ª–Ω—ã–π —Ä–∞–∑–º–µ—Ä " + str(len(file_data.get('text', ''))) + " —Å–∏–º–≤–æ–ª–æ–≤)"
                        context_str += f"- PDF: {filename}\n--- –°–û–î–ï–†–ñ–ò–ú–û–ï PDF ---\n{pdf_text}\n--- –ö–û–ù–ï–¶ PDF ---\n"
                    elif file_type in ("application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                      "application/msword") and 'text' in file_data:
                        docx_text = file_data.get('text', '')
                        # Truncate if too long - increased limit for better analysis
                        max_len = 10000
                        if len(docx_text) > max_len:
                            docx_text = docx_text[:max_len] + "\n... (—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω, –ø–æ–ª–Ω—ã–π —Ä–∞–∑–º–µ—Ä " + str(len(file_data.get('text', ''))) + " —Å–∏–º–≤–æ–ª–æ–≤)"
                        context_str += f"- Word –¥–æ–∫—É–º–µ–Ω—Ç: {filename}\n--- –°–û–î–ï–†–ñ–ò–ú–û–ï DOCX ---\n{docx_text}\n--- –ö–û–ù–ï–¶ DOCX ---\n"
                    else:
                        context_str += f"- {filename} ({file_type})\n"
                
                if has_images and model_supports_vision:
                    context_str += "\n‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ù–û: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –£–ñ–ï –ü–ï–†–ï–î–ê–ù–´ –≤ —ç—Ç–æ–º —Å–æ–æ–±—â–µ–Ω–∏–∏ —á–µ—Ä–µ–∑ Vision API! –¢—ã –≤–∏–¥–∏—à—å –∏—Ö –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å! –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –∏—Ö –∞–Ω–∞–ª–∏–∑–∞ - –ø—Ä–æ—Å—Ç–æ –æ–ø–∏—à–∏ —á—Ç–æ –≤–∏–¥–∏—à—å –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö!\n"
                else:
                    context_str += "‚ö†Ô∏è –ù–ï –∏—â–∏ —ç—Ç–∏ —Ñ–∞–π–ª—ã –≤ Google Drive - –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –£–ñ–ï –í–´–®–ï!\n"
        
        # Add open files context (PRIORITY #2)
        open_files = context.get_open_files() if hasattr(context, 'get_open_files') else []
        if open_files:
            context_str += "\nüìÇ –û—Ç–∫—Ä—ã—Ç—ã–µ —Ñ–∞–π–ª—ã –≤ —Ä–∞–±–æ—á–µ–π –æ–±–ª–∞—Å—Ç–∏ (–ü–†–ò–û–†–ò–¢–ï–¢ #2):\n"
            for file in open_files:
                if file.get('type') == 'sheets':
                    context_str += f"- –¢–∞–±–ª–∏—Ü–∞: {file.get('title')} (ID: {file.get('spreadsheet_id')})\n"
                elif file.get('type') == 'docs':
                    context_str += f"- –î–æ–∫—É–º–µ–Ω—Ç: {file.get('title')} (ID: {file.get('document_id')})\n"
            context_str += "‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–π document_id/spreadsheet_id –Ω–∞–ø—Ä—è–º—É—é, –ù–ï –∏—â–∏ —á–µ—Ä–µ–∑ search!\n"
        
        prompt = f"""–¢—ã –ø–ª–∞–Ω–∏—Ä—É–µ—à—å —Å–ª–µ–¥—É—é—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏.

{context_str}

–î–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
{tools_str}

–í–ê–ñ–ù–û:
- –ï—Å–ª–∏ –ø–æ–ª—É—á–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–æ–±—ã—Ç–∏–π/–¥–∞–Ω–Ω—ã—Ö, –Ω–æ –ë–ï–ó –¥–µ—Ç–∞–ª–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, "Found 10 events" –±–µ–∑ —Å–ø–∏—Å–∫–∞), 
  —ç—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –î–ï–¢–ê–õ–ò —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- –ù–ï –∑–∞–≤–µ—Ä—à–∞–π –∑–∞–¥–∞—á—É, –ø–æ–∫–∞ –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–µ—Ç–∞–ª–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
- –î–ª—è –∫–∞–ª–µ–Ω–¥–∞—Ä—è: –µ—Å–ª–∏ –ø–æ–ª—É—á–µ–Ω–æ —Ç–æ–ª—å–∫–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π, –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª–∏ –∫–∞–∂–¥–æ–≥–æ —Å–æ–±—ã—Ç–∏—è
- –î–ª—è —Ñ–∞–π–ª–æ–≤: –µ—Å–ª–∏ –ø–æ–ª—É—á–µ–Ω —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤, –Ω–æ –Ω—É–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ - –ø–æ–ª—É—á–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
- –î–ª—è –ø–∏—Å–µ–º: –µ—Å–ª–∏ –ø–æ–ª—É—á–µ–Ω —Å–ø–∏—Å–æ–∫ –ø–∏—Å–µ–º, –Ω–æ –Ω—É–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ - –ø–æ–ª—É—á–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ

–ö–†–ò–¢–ò–ß–ù–û –î–õ–Ø –ü–†–ò–ö–†–ï–ü–õ–ï–ù–ù–´–• –§–ê–ô–õ–û–í:
- –ï—Å–ª–∏ –≤ –ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö –µ—Å—Ç—å –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø - –æ–Ω–∏ –£–ñ–ï –ü–ï–†–ï–î–ê–ù–´ –≤ —ç—Ç–æ–º —Å–æ–æ–±—â–µ–Ω–∏–∏ —á–µ—Ä–µ–∑ Vision API! 
  –¢—ã –≤–∏–¥–∏—à—å –∏—Ö –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å! –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Ç–∏–ø–∞ "vision-api" –∏–ª–∏ "analyze_image" - –ø—Ä–æ—Å—Ç–æ –æ–ø–∏—à–∏ —á—Ç–æ –≤–∏–¥–∏—à—å!
- –ï—Å–ª–∏ –≤ –ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö –µ—Å—Ç—å PDF –∏–ª–∏ DOCX - –∏—Ö –¢–ï–ö–°–¢ –£–ñ–ï –ü–†–ï–î–°–¢–ê–í–õ–ï–ù –í–´–®–ï –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ!
- –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç "—á—Ç–æ –≤ —Ñ–∞–π–ª–µ" –∏–ª–∏ "—á—Ç–æ –≤ —Ñ–∞–π–ª–∞—Ö" –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–æ–≤ –£–ñ–ï –í–ò–î–ù–û (—Ç–µ–∫—Å—Ç PDF/DOCX –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤—ã—à–µ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Vision API), 
  —Ç–æ –∑–∞–¥–∞—á–∞ –£–ñ–ï –í–´–ü–û–õ–ù–ï–ù–ê - –∏—Å–ø–æ–ª—å–∑—É–π FINISH –∏ –æ–ø–∏—à–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ!
- –ù–ï –∏—â–∏ —Ñ–∞–π–ª—ã –≤ Google Drive –∏–ª–∏ —Ä–∞–±–æ—á–µ–π –æ–±–ª–∞—Å—Ç–∏, –µ—Å–ª–∏ –æ–Ω–∏ —É–∂–µ –ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω—ã –∏ –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —É–∂–µ –≤–∏–¥–Ω–æ!

–í—ã–±–µ—Ä–∏ –û–î–ò–ù –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∏ —É–∫–∞–∂–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –µ–≥–æ –≤—ã–∑–æ–≤–∞. –û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "tool_name": "–∏–º—è_–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞",
    "arguments": {{"param1": "value1", "param2": "value2"}},
    "description": "–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è",
    "reasoning": "–ø–æ—á–µ–º—É –≤—ã–±—Ä–∞–Ω–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏–µ"
}}

–ï—Å–ª–∏ —Ü–µ–ª—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ –∏ –ø–æ–ª—É—á–µ–Ω—ã –í–°–ï –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–µ—Ç–∞–ª–∏ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, –∏—Å–ø–æ–ª—å–∑—É–π —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –º–∞—Ä–∫–µ—Ä:
{{
    "tool_name": "FINISH",
    "arguments": {{}},
    "description": "–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏",
    "reasoning": "–ø–æ—á–µ–º—É –∑–∞–¥–∞—á–∞ —Å—á–∏—Ç–∞–µ—Ç—Å—è –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–π (—É–∫–∞–∂–∏, –∫–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã)"
}}

–û–°–û–ë–ï–ù–ù–û: –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º –ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤, –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –£–ñ–ï –í–ò–î–ù–û (—Ç–µ–∫—Å—Ç PDF/DOCX –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤—ã—à–µ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Vision API), 
–∏—Å–ø–æ–ª—å–∑—É–π FINISH –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ - –Ω–µ –∏—â–∏ —Ñ–∞–π–ª—ã –≤ –¥—Ä—É–≥–∏—Ö –º–µ—Å—Ç–∞—Ö!

–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É vision –∏ —Å–æ–±–∏—Ä–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            model_supports_vision = supports_vision(self.model_name) if self.model_name else False
            image_contents = []
            
            if file_ids and model_supports_vision:
                for file_id in file_ids:
                    file_data = context.get_file(file_id)
                    if file_data:
                        file_type = file_data.get('type', '')
                        if file_type.startswith('image/'):
                            media_type = file_data.get('media_type', file_type)
                            base64_data = file_data.get('data', '')
                            if base64_data:
                                image_contents.append({
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:{media_type};base64,{base64_data}"
                                    }
                                })
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
            if image_contents:
                # Multimodal —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
                message_content = [{"type": "text", "text": prompt}] + image_contents
                messages = [
                    SystemMessage(content="–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –¥–µ–π—Å—Ç–≤–∏–π. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–º JSON."),
                    HumanMessage(content=message_content)
                ]
            else:
                # –û–±—ã—á–Ω–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                messages = [
                    SystemMessage(content="–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –¥–µ–π—Å—Ç–≤–∏–π. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–º JSON."),
                    HumanMessage(content=prompt)
                ]
            
            response = await self.llm.ainvoke(messages)
            
            # Handle different response formats
            if isinstance(response.content, list):
                text_parts = []
                for block in response.content:
                    if hasattr(block, "text"):
                        text_parts.append(block.text)
                    elif isinstance(block, dict) and "text" in block:
                        text_parts.append(block["text"])
                    elif isinstance(block, str):
                        text_parts.append(block)
                response_text = " ".join(text_parts).strip()
            elif isinstance(response.content, str):
                response_text = response.content.strip()
            else:
                response_text = str(response.content).strip()
            
            # Extract JSON
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                json_str = json_match.group(0)
                action_plan = json.loads(json_str)
            else:
                action_plan = json.loads(response_text)
            
            # Validate
            if "tool_name" not in action_plan:
                raise ValueError("tool_name missing in action plan")
            
            return action_plan
            
        except Exception as e:
            logger.error(f"[UnifiedReActEngine] Error in _plan_action: {e}")
            # Fallback
            if self.capabilities:
                fallback_cap = self.capabilities[0]
                return {
                    "tool_name": fallback_cap.name,
                    "arguments": {},
                    "description": f"Fallback: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ {fallback_cap.name}",
                    "reasoning": f"–û—à–∏–±–∫–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {str(e)}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç."
                }
            else:
                return {
                    "tool_name": "error",
                    "arguments": {},
                    "description": "–û—à–∏–±–∫–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤",
                    "reasoning": str(e)
                }
    
    async def _execute_action(
        self,
        action_plan: Dict[str, Any],
        context: ConversationContext
    ) -> Any:
        """Execute action through CapabilityRegistry (provider-agnostic)."""
        capability_name = action_plan.get("tool_name")
        arguments = action_plan.get("arguments", {})
        
        # Registry routes to appropriate provider (MCP or A2A)
        return await self.registry.execute(capability_name, arguments)
    
    async def _find_alternative(
        self,
        state: ReActState,
        analysis: Analysis,
        context: ConversationContext,
        file_ids: List[str]
    ) -> Optional[Dict[str, Any]]:
        """Find alternative action when current one failed."""
        context_str = f"–¶–µ–ª—å: {state.goal}\n\n"
        context_str += f"–û—à–∏–±–∫–∞: {analysis.error_message}\n\n"
        context_str += "–ù–µ—É–¥–∞—á–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏:\n"
        for action in state.action_history[-3:]:
            context_str += f"- {action.tool_name}\n"
        
        context_str += f"\n–ò—Å–ø—Ä–æ–±–æ–≤–∞–Ω–Ω—ã–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã: {', '.join(state.alternatives_tried) if state.alternatives_tried else '–Ω–µ—Ç'}\n"
        
        # Get capability descriptions
        capability_descriptions = []
        for cap in self.capabilities[:50]:
            capability_descriptions.append(f"- {cap.name}: {cap.description}")
        
        tools_str = "\n".join(capability_descriptions)
        
        prompt = f"""–ü—Ä–µ–¥—ã–¥—É—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å. –ù–∞–π–¥–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏.

{context_str}

–î–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
{tools_str}

–ü—Ä–µ–¥–ª–æ–∂–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "tool_name": "–∏–º—è_–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞",
    "arguments": {{"param1": "value1"}},
    "description": "–æ–ø–∏—Å–∞–Ω–∏–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è",
    "reasoning": "–ø–æ—á–µ–º—É —ç—Ç–æ –¥–æ–ª–∂–Ω–æ —Å—Ä–∞–±–æ—Ç–∞—Ç—å"
}}

–ï—Å–ª–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã –Ω–µ—Ç, –≤–µ—Ä–Ω–∏ {{"alternative": false}}.

–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON."""

        try:
            messages = [
                SystemMessage(content="–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–æ–∏—Å–∫—É –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–º JSON."),
                HumanMessage(content=prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            
            # Handle different response formats
            if isinstance(response.content, list):
                text_parts = []
                for block in response.content:
                    if hasattr(block, "text"):
                        text_parts.append(block.text)
                    elif isinstance(block, dict) and "text" in block:
                        text_parts.append(block["text"])
                    elif isinstance(block, str):
                        text_parts.append(block)
                response_text = " ".join(text_parts).strip()
            elif isinstance(response.content, str):
                response_text = response.content.strip()
            else:
                response_text = str(response.content).strip()
            
            # Extract JSON
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                json_str = json_match.group(0)
                alternative = json.loads(json_str)
            else:
                alternative = json.loads(response_text)
            
            # Check if alternative exists
            if alternative.get("alternative") is False:
                return None
            
            if "tool_name" not in alternative:
                return None
            
            return alternative
            
        except Exception as e:
            logger.error(f"[UnifiedReActEngine] Error in _find_alternative: {e}")
            return None
    
    async def _generate_final_answer(self, state: ReActState, context: Optional[ConversationContext] = None, file_ids: Optional[List[str]] = None) -> str:
        """Generate a human-friendly final answer based on all collected results with streaming."""
        try:
            # Collect all observations/results
            observations_text = ""
            for obs in state.observations:
                if obs.raw_result:
                    observations_text += f"- {obs.action.tool_name}: {str(obs.raw_result)[:1500]}\n"
            
            # If no observations but we have FINISH reasoning, use it
            if not observations_text:
                # Check for FINISH marker in reasoning trail
                for step in reversed(state.reasoning_trail):
                    if step.metadata and step.metadata.get("tool") == "FINISH":
                        # Use the reasoning from FINISH step
                        observations_text = step.content
                        break
            
            if not observations_text:
                observations_text = "–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤."
            
            # Build file contents for FINISH cases
            file_contents_text = ""
            if file_ids and context:
                for file_id in file_ids:
                    file_data = context.get_file(file_id)
                    if file_data:
                        filename = file_data.get('filename', 'unknown')
                        file_type = file_data.get('type', '')
                        full_text = file_data.get('text', '')
                        # Use larger limit for final answer - user wants detailed description
                        max_len = 15000
                        if file_type == 'application/pdf' and 'text' in file_data:
                            pdf_text = full_text[:max_len] if len(full_text) > max_len else full_text
                            truncation_note = f"\n... (–ø–æ–∫–∞–∑–∞–Ω–æ {max_len} –∏–∑ {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤)" if len(full_text) > max_len else ""
                            file_contents_text += f"\nüìÑ PDF '{filename}':\n{pdf_text}{truncation_note}\n"
                        elif file_type in ("application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                          "application/msword") and 'text' in file_data:
                            docx_text = full_text[:max_len] if len(full_text) > max_len else full_text
                            truncation_note = f"\n... (–ø–æ–∫–∞–∑–∞–Ω–æ {max_len} –∏–∑ {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤)" if len(full_text) > max_len else ""
                            file_contents_text += f"\nüìÑ Word '{filename}':\n{docx_text}{truncation_note}\n"
                        elif file_type.startswith('image/'):
                            file_contents_text += f"\nüñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ '{filename}': (–ø–µ—Ä–µ–¥–∞–Ω–æ —á–µ—Ä–µ–∑ Vision API - –æ–ø–∏—à–∏ —á—Ç–æ –≤–∏–¥–∏—à—å)\n"
            
            # Check if user asked for a table
            goal_lower = state.goal.lower()
            wants_table = any(word in goal_lower for word in ['—Ç–∞–±–ª–∏—á–∫', '—Ç–∞–±–ª–∏—Ü', 'table'])
            
            if wants_table:
                table_instruction = """
–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
–í—ã–≤–µ–¥–∏ –¥–∞–Ω–Ω—ã–µ –≤ –≤–∏–¥–µ MARKDOWN –¢–ê–ë–õ–ò–¶–´. –ü—Ä–∏–º–µ—Ä:
| –ù–∞–∑–≤–∞–Ω–∏–µ | –î–∞—Ç–∞ | –í—Ä–µ–º—è |
|----------|------|-------|
| –í—Å—Ç—Ä–µ—á–∞ 1 | 2025-12-25 | 10:00 |

–ü–æ—Å–ª–µ —Ç–∞–±–ª–∏—Ü—ã –¥–æ–±–∞–≤—å –ø—Ä–∏–º–µ—á–∞–Ω–∏–µ:
"üí° –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å Google —Ç–∞–±–ª–∏—Ü—É —Å —ç—Ç–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏, –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –≤ —Ä–µ–∂–∏–º **–ê–≥–µ–Ω—Ç**."
"""
            else:
                table_instruction = ""
            
            # Check if this is a FINISH case (reasoning contains file analysis)
            is_finish_case = any(
                step.metadata and step.metadata.get("tool") == "FINISH"
                for step in state.reasoning_trail
            )
            
            if is_finish_case and file_contents_text:
                # For FINISH with file content, include actual file contents in prompt
                prompt = f"""–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–æ—Å–∏–ª: "{state.goal}"

–í–æ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤:
{file_contents_text}

{table_instruction}
–í–ê–ñ–ù–û: –û–ø–∏—à–∏ –ö–û–ù–ö–†–ï–¢–ù–û —á—Ç–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ñ–∞–π–ª–∞—Ö. –ù–∞–ø—Ä–∏–º–µ—Ä:
- –î–ª—è PDF: "–í —Ñ–∞–π–ª–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —á–µ–∫ –Ω–∞ –æ–ø–ª–∞—Ç—É –Ω–∞–ª–æ–≥–æ–≤ –Ω–∞ —Å—É–º–º—É X —Ä—É–±. –æ—Ç –¥–∞—Ç—ã Y..."
- –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: "–ù–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –ø–æ–∫–∞–∑–∞–Ω —á–µ–ª–æ–≤–µ–∫, –∏–≥—Ä–∞—é—â–∏–π –≤ —Ç–µ–Ω–Ω–∏—Å..."
–ù–ï –≥–æ–≤–æ—Ä–∏ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ "—Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é". –ë—É–¥—å –ö–û–ù–ö–†–ï–¢–ù–´–ú!

–û—Ç–≤–µ—Ç:"""
            elif is_finish_case:
                # FINISH case without file contents - use reasoning
                prompt = f"""–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–æ—Å–∏–ª: "{state.goal}"

–ê–Ω–∞–ª–∏–∑:
{observations_text}

{table_instruction}
–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –ø–æ–Ω—è—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –æ–ø–∏—Å—ã–≤–∞—è —á—Ç–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ñ–∞–π–ª–µ/—Ñ–∞–π–ª–∞—Ö. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º.

–û—Ç–≤–µ—Ç:"""
            else:
                prompt = f"""–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{state.goal}"

–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:
{observations_text}

–í–ê–ñ–ù–û: –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã—à–µ. –ï—Å–ª–∏ —Ç–∞–º –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ (events, messages, files –∏ —Ç.–¥.) - –∑–Ω–∞—á–∏—Ç –æ–Ω–∏ –ù–ê–ô–î–ï–ù–´.
{table_instruction}
–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –æ—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:
- –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ - –ø–µ—Ä–µ—á–∏—Å–ª–∏ –∏—Ö –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ
- –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã–µ (–ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ [], "Found 0") - —Å–∫–∞–∂–∏ —á—Ç–æ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
- –ù–ï –≥–æ–≤–æ—Ä–∏ —á—Ç–æ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –µ—Å–ª–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –µ—Å—Ç—å –∑–∞–ø–∏—Å–∏!

–û—Ç–≤–µ—Ç:"""

            # Build multimodal message with images if available
            image_contents = []
            model_supports_vision = supports_vision(self.model_name) if self.model_name else False
            
            if file_ids and context and model_supports_vision:
                for file_id in file_ids:
                    file_data = context.get_file(file_id)
                    if file_data:
                        file_type = file_data.get('type', '')
                        if file_type.startswith('image/'):
                            media_type = file_data.get('media_type', file_type)
                            base64_data = file_data.get('data', '')
                            if base64_data:
                                image_contents.append({
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:{media_type};base64,{base64_data}"
                                    }
                                })
            
            # Create message (multimodal if images present)
            if image_contents:
                message_content = [{"type": "text", "text": prompt}] + image_contents
                messages = [HumanMessage(content=message_content)]
            else:
                messages = [HumanMessage(content=prompt)]

            # Stream the response
            full_answer = ""
            
            # Send intent event to show user what's happening
            intent_message = "–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–æ–≤" if file_contents_text else "–§–æ—Ä–º–∏—Ä—É—é –æ—Ç–≤–µ—Ç"
            if len(image_contents) > 0:
                intent_message += f" (–≤–∫–ª—é—á–∞—è {len(image_contents)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ(—è))..."
            else:
                intent_message += "..."
            
            intent_id = f"intent-final-{int(time.time() * 1000)}"
            await self.ws_manager.send_event(
                self.session_id,
                "intent_start",
                {"intent_id": intent_id, "text": intent_message}  # Fixed: use 'text' not 'intent'
            )
            
            # Send details about each file being analyzed
            if file_ids and context:
                for i, file_id in enumerate(file_ids):
                    file_data = context.get_file(file_id)
                    if file_data:
                        filename = file_data.get('filename', 'unknown')
                        file_type = file_data.get('type', '')
                        detail_type = 'read'
                        if file_type.startswith('image/'):
                            detail_desc = f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {filename}"
                        elif 'pdf' in file_type:
                            detail_desc = f"–ß–∏—Ç–∞—é PDF: {filename}"
                        elif 'word' in file_type or 'document' in file_type:
                            detail_desc = f"–ß–∏—Ç–∞—é –¥–æ–∫—É–º–µ–Ω—Ç: {filename}"
                        else:
                            detail_desc = f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª: {filename}"
                        
                        await self.ws_manager.send_event(
                            self.session_id,
                            "intent_detail",
                            {
                                "intent_id": intent_id,
                                "type": detail_type,
                                "description": detail_desc
                            }
                        )
            
            # Send start event
            await self.ws_manager.send_event(
                self.session_id,
                "final_result_start",
                {}
            )
            
            # Stream chunks
            async for chunk in self.llm.astream(messages):
                chunk_text = ""
                if hasattr(chunk, 'content') and chunk.content:
                    content = chunk.content
                    # Handle multimodal response where content is a list
                    if isinstance(content, list):
                        for block in content:
                            if hasattr(block, 'text'):
                                chunk_text += block.text
                            elif isinstance(block, dict) and 'text' in block:
                                chunk_text += block['text']
                            elif isinstance(block, str):
                                chunk_text += block
                    elif isinstance(content, str):
                        chunk_text = content
                elif isinstance(chunk, str):
                    chunk_text = chunk
                
                if chunk_text:
                    full_answer += chunk_text
                    await self.ws_manager.send_event(
                        self.session_id,
                        "final_result_chunk",
                        {"content": full_answer}  # Send accumulated content
                    )
            
            # Send intent completion
            await self.ws_manager.send_event(
                self.session_id,
                "intent_complete",
                {"intent_id": intent_id, "summary": "–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω"}
            )
            
            # Send completion event
            await self.ws_manager.send_event(
                self.session_id,
                "final_result_complete",
                {"content": full_answer.strip()}
            )
            
            return full_answer.strip()
        except Exception as e:
            logger.error(f"[UnifiedReActEngine] Error generating final answer: {e}")
            # Fallback to last result
            if state.observations:
                last_result = str(state.observations[-1].raw_result)
                return self._format_result_summary(last_result, state.observations[-1].action.tool_name)
            return "–ó–∞–¥–∞—á–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞."

    async def _finalize_success(
        self,
        state: ReActState,
        final_result: Any,
        context: ConversationContext,
        file_ids: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """Finalize successful execution."""
        state.status = "done"
        
        # Generate human-friendly final answer instead of raw result
        human_answer = await self._generate_final_answer(state, context, file_ids)
        
        result_summary = {
            "status": "completed",
            "goal": state.goal,
            "iterations": state.iteration,
            "actions_taken": len(state.action_history),
            "final_result": human_answer,
            "reasoning_trail": [
                {
                    "iteration": step.iteration,
                    "type": step.step_type,
                    "content": step.content,
                    "metadata": step.metadata
                }
                for step in state.reasoning_trail
            ]
        }
        
        # Send thinking_completed event FIRST (before final_result to stop animations)
        if self._current_thinking_id and self._thinking_start_time:
            elapsed_seconds = time.time() - self._thinking_start_time
            # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ reasoning trail
            full_content = "\n".join([step.content for step in state.reasoning_trail])
            await self.ws_manager.send_event(
                self.session_id,
                "thinking_completed",
                {
                    "thinking_id": self._current_thinking_id,
                    "full_content": full_content,
                    "elapsed_seconds": elapsed_seconds,
                    "auto_collapse": True
                }
            )
            self._current_thinking_id = None
            self._thinking_start_time = None
        
        # Send react_complete event
        await self.ws_manager.send_event(
            self.session_id,
            "react_complete",
            {
                "result": human_answer[:1000],
                "trail": result_summary["reasoning_trail"][-10:]
            }
        )
        
        # Send final_result or message_complete event based on mode
        # NOTE: final_result_start, final_result_chunk, final_result_complete are already sent by _generate_final_answer
        # So we only send final_result here as a final confirmation (or skip if already sent)
        if self.config.mode == "query":
            # For query mode, send workflow_stopped to indicate completion (stops animations)
            await self.ws_manager.send_event(
                self.session_id,
                "workflow_stopped",
                {
                    "reason": "–ó–∞–¥–∞—á–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞"
                }
            )
        else:
            # For agent and plan modes, send message_complete to ensure response is displayed
            message_id = f"react_{self.session_id}_{int(time.time() * 1000)}"
            await self.ws_manager.send_event(
                self.session_id,
                "message_complete",
                {
                    "role": "assistant",
                    "message_id": message_id,
                    "content": human_answer
                }
            )
        
        if hasattr(context, 'add_message'):
            context.add_message("assistant", f"–ó–∞–¥–∞—á–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞: {state.goal}")
        
        logger.info(f"[UnifiedReActEngine] Successfully completed in {state.iteration} iterations")
        return result_summary
    
    async def _finalize_failure(
        self,
        state: ReActState,
        analysis: Analysis,
        context: ConversationContext
    ) -> Dict[str, Any]:
        """Finalize failed execution with report."""
        state.status = "failed"
        
        failure_report = {
            "status": "failed",
            "goal": state.goal,
            "iterations": state.iteration,
            "actions_taken": len(state.action_history),
            "error": analysis.error_message or "–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ—Å—Ç–∏—á—å —Ü–µ–ª–∏",
            "alternatives_tried": state.alternatives_tried,
            "reasoning_trail": [
                {
                    "iteration": step.iteration,
                    "type": step.step_type,
                    "content": step.content,
                    "metadata": step.metadata
                }
                for step in state.reasoning_trail
            ]
        }
        
        await self.ws_manager.send_event(
            self.session_id,
            "react_failed",
            {
                "reason": failure_report["error"],
                "tried": state.alternatives_tried
            }
        )
        
        # Send message_complete with error message for agent/plan modes
        if self.config.mode != "query":
            error_message = f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–¥–∞—á—É: {failure_report['error']}"
            message_id = f"react_{self.session_id}_{int(time.time() * 1000)}"
            await self.ws_manager.send_event(
                self.session_id,
                "message_complete",
                {
                    "role": "assistant",
                    "message_id": message_id,
                    "content": error_message
                }
            )
        
        # Send thinking_completed event (with error, –Ω–µ —Å–≤–æ—Ä–∞—á–∏–≤–∞–µ–º)
        if self._current_thinking_id and self._thinking_start_time:
            elapsed_seconds = time.time() - self._thinking_start_time
            full_content = "\n".join([step.content for step in state.reasoning_trail])
            await self.ws_manager.send_event(
                self.session_id,
                "thinking_completed",
                {
                    "thinking_id": self._current_thinking_id,
                    "full_content": full_content,
                    "elapsed_seconds": elapsed_seconds,
                    "auto_collapse": False  # –ù–µ —Å–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –ø—Ä–∏ –æ—à–∏–±–∫–µ
                }
            )
            self._current_thinking_id = None
            self._thinking_start_time = None
        
        logger.warning(f"[UnifiedReActEngine] Failed after {state.iteration} iterations: {failure_report['error']}")
        return failure_report
    
    async def _finalize_timeout(
        self,
        state: ReActState,
        context: ConversationContext
    ) -> Dict[str, Any]:
        """Finalize execution that reached max iterations."""
        state.status = "failed"
        
        timeout_report = {
            "status": "timeout",
            "goal": state.goal,
            "iterations": state.iteration,
            "actions_taken": len(state.action_history),
            "message": f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –∏—Ç–µ—Ä–∞—Ü–∏–π ({state.max_iterations})",
            "reasoning_trail": [
                {
                    "iteration": step.iteration,
                    "type": step.step_type,
                    "content": step.content,
                    "metadata": step.metadata
                }
                for step in state.reasoning_trail
            ]
        }
        
        await self.ws_manager.send_event(
            self.session_id,
            "react_failed",
            {
                "reason": timeout_report["message"],
                "tried": state.alternatives_tried
            }
        )
        
        # Send timeout message based on mode
        timeout_message = f"‚è±Ô∏è {timeout_report['message']}. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É—Ç–æ—á–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ —Ä–∞–∑–±–∏—Ç—å –∑–∞–¥–∞—á—É –Ω–∞ –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ —à–∞–≥–∏."
        
        if self.config.mode == "query":
            # For Query mode, send final_result event
            await self.ws_manager.send_event(
                self.session_id,
                "final_result",
                {
                    "content": timeout_message
                }
            )
        else:
            # For agent and plan modes, send message_complete
            message_id = f"react_{self.session_id}_{int(time.time() * 1000)}"
            await self.ws_manager.send_event(
                self.session_id,
                "message_complete",
                {
                    "role": "assistant",
                    "message_id": message_id,
                    "content": timeout_message
                }
            )
        
        # Send thinking_completed event (timeout)
        if self._current_thinking_id and self._thinking_start_time:
            elapsed_seconds = time.time() - self._thinking_start_time
            full_content = "\n".join([step.content for step in state.reasoning_trail])
            await self.ws_manager.send_event(
                self.session_id,
                "thinking_completed",
                {
                    "thinking_id": self._current_thinking_id,
                    "full_content": full_content,
                    "elapsed_seconds": elapsed_seconds,
                    "auto_collapse": False
                }
            )
            self._current_thinking_id = None
            self._thinking_start_time = None
        
        logger.warning(f"[UnifiedReActEngine] Timeout after {state.iteration} iterations")
        return timeout_report
    
    def _transform_to_human_readable(self, action: str, tool_name: str) -> str:
        """Transform technical messages to human-readable format."""
        action_lower = action.lower()
        tool_lower = tool_name.lower()
        
        # –ï—Å–ª–∏ —É–∂–µ human-readable, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        if not action_lower.startswith(('fallback:', 'error:', '–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ')):
            return action
        
        # –ú–∞–ø–ø–∏–Ω–≥ tool names –Ω–∞ human-readable –æ–ø–∏—Å–∞–Ω–∏—è
        if 'calendar' in tool_lower or 'event' in tool_lower:
            return "üìÖ –ü–æ–ª—É—á–∞—é —Å–æ–±—ã—Ç–∏—è –∫–∞–ª–µ–Ω–¥–∞—Ä—è..."
        elif 'email' in tool_lower or 'gmail' in tool_lower or 'mail' in tool_lower:
            return "üìß –ò—â—É –≤ –ø–æ—á—Ç–µ..."
        elif 'file' in tool_lower or 'workspace' in tool_lower or 'drive' in tool_lower:
            return "üìÅ –ò—â—É —Ñ–∞–π–ª—ã..."
        elif 'search' in tool_lower:
            return "üîç –ò—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é..."
        elif 'create' in tool_lower or 'write' in tool_lower:
            return "‚úèÔ∏è –°–æ–∑–¥–∞—é –¥–æ–∫—É–º–µ–Ω—Ç..."
        elif 'read' in tool_lower or 'get' in tool_lower:
            return "üìñ –ß–∏—Ç–∞—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é..."
        else:
            return "üîß –í—ã–ø–æ–ª–Ω—è—é –¥–µ–π—Å—Ç–≤–∏–µ..."
    
    def _get_detail_type(self, tool_name: str) -> str:
        """Map tool name to intent detail type."""
        tool_lower = tool_name.lower()
        if 'search' in tool_lower or 'find' in tool_lower:
            return 'search'
        elif 'read' in tool_lower or 'get' in tool_lower or 'list' in tool_lower or 'fetch' in tool_lower:
            return 'read'
        elif 'create' in tool_lower or 'write' in tool_lower or 'send' in tool_lower or 'update' in tool_lower:
            return 'write'
        else:
            return 'execute'
    
    def _extract_result_details(self, result: str) -> List[str]:
        """Extract meaningful details from result for display in intent block."""
        details = []
        try:
            import json
            import re
            logger.debug(f"[_extract_result_details] Parsing result: {result[:200]}...")
            
            # Try to parse as JSON
            data = None
            if result.strip().startswith('{') or result.strip().startswith('['):
                try:
                    data = json.loads(result)
                except json.JSONDecodeError:
                    pass
            
            if isinstance(data, list):
                # List of items (events, messages, files)
                logger.debug(f"[_extract_result_details] Found list with {len(data)} items")
                for item in data[:10]:  # Max 10 items
                    if isinstance(item, dict):
                        name = item.get('summary') or item.get('title') or item.get('subject') or item.get('name') or item.get('filename')
                        start = item.get('start', {})
                        time_str = ""
                        if isinstance(start, dict):
                            time_str = start.get('dateTime', start.get('date', ''))[:16].replace('T', ' ')
                        elif isinstance(start, str):
                            time_str = start[:16].replace('T', ' ')
                        if name:
                            if time_str:
                                details.append(f"üìÖ {name} - {time_str}")
                            else:
                                details.append(f"‚Ä¢ {name}")
            elif isinstance(data, dict):
                logger.debug(f"[_extract_result_details] Found dict with keys: {list(data.keys())[:10]}")
                if 'events' in data:
                    for event in data['events'][:10]:
                        name = event.get('summary') or event.get('title')
                        start = event.get('start', {})
                        time_str = ""
                        if isinstance(start, dict):
                            time_str = start.get('dateTime', start.get('date', ''))[:16].replace('T', ' ')
                        if name:
                            details.append(f"üìÖ {name} - {time_str}" if time_str else f"üìÖ {name}")
                elif 'messages' in data:
                    for msg in data['messages'][:10]:
                        subject = msg.get('subject') or msg.get('snippet', '')[:50]
                        if subject:
                            details.append(f"üìß {subject}")
                elif 'files' in data:
                    for f in data['files'][:10]:
                        name = f.get('name') or f.get('title')
                        if name:
                            details.append(f"üìÑ {name}")
                else:
                    name = data.get('summary') or data.get('title') or data.get('subject')
                    if name:
                        details.append(f"‚Ä¢ {name}")
            
            # If no structured data found, check for "Found N event(s)" pattern - parse calendar format
            if not details and 'Found' in result and 'event' in result.lower():
                lines = result.split('\n')
                current_event_name = None
                current_event_time = None
                
                for line in lines:
                    line = line.strip()
                    # Match event number and name: "1. –ø—Ä–æ–≤–µ—Ä–∫–∞ 1"
                    event_match = re.match(r'^(\d+)\.\s*(.+)$', line)
                    if event_match:
                        # Save previous event if exists
                        if current_event_name:
                            if current_event_time:
                                details.append(f"üìÖ {current_event_name} - {current_event_time}")
                            else:
                                details.append(f"üìÖ {current_event_name}")
                        current_event_name = event_match.group(2).strip()
                        current_event_time = None
                    # Match time line: "–í—Ä–µ–º—è: 2025-12-25 05:00 - 2025-12-25 06:00"
                    elif line.startswith('–í—Ä–µ–º—è:') or line.startswith('Time:'):
                        time_part = line.split(':', 1)[1].strip()
                        # Extract just date and start time
                        time_match = re.match(r'(\d{4}-\d{2}-\d{2})\s*(\d{2}:\d{2})?', time_part)
                        if time_match:
                            current_event_time = f"{time_match.group(1)} {time_match.group(2) or ''}".strip()
                    
                    if len(details) >= 10:
                        break
                
                # Don't forget the last event
                if current_event_name and len(details) < 10:
                    if current_event_time:
                        details.append(f"üìÖ {current_event_name} - {current_event_time}")
                    else:
                        details.append(f"üìÖ {current_event_name}")
                        
        except Exception as e:
            logger.error(f"[_extract_result_details] Error: {e}")
            lines = result.split('\n')
            for line in lines[:5]:
                line = line.strip()
                if line and len(line) > 3 and not line.startswith('{'):
                    details.append(f"‚Ä¢ {line[:100]}")
        
        logger.debug(f"[_extract_result_details] Extracted {len(details)} details: {details}")
        return details

    def _format_result_summary(self, result: str, tool: str) -> str:
        """Format raw tool result into human-readable Russian summary."""
        import re
        result_lower = result.lower()
        tool_lower = tool.lower() if tool else ""
        
        # Extract count from common patterns like "Found 5 events", "Found 0 messages"
        count_match = re.search(r'found\s+(\d+)\s+(\w+)', result_lower)
        if count_match:
            count = int(count_match.group(1))
            item_type = count_match.group(2)
            
            # Map item types to Russian with proper pluralization
            def pluralize_ru(n: int, one: str, few: str, many: str) -> str:
                mod10 = n % 10
                mod100 = n % 100
                if mod100 >= 11 and mod100 <= 14:
                    return many
                if mod10 == 1:
                    return one
                if mod10 >= 2 and mod10 <= 4:
                    return few
                return many
            
            if 'event' in item_type or 'calendar' in item_type or '–≤—Å—Ç—Ä–µ—á' in tool_lower:
                word = pluralize_ru(count, '–≤—Å—Ç—Ä–µ—á–∞', '–≤—Å—Ç—Ä–µ—á–∏', '–≤—Å—Ç—Ä–µ—á')
                return f"–ù–∞–π–¥–µ–Ω–æ {count} {word}" if count > 0 else "–í—Å—Ç—Ä–µ—á –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
            elif 'message' in item_type or 'mail' in item_type or 'email' in item_type or '–ø–∏—Å—å–º' in tool_lower:
                word = pluralize_ru(count, '–ø–∏—Å—å–º–æ', '–ø–∏—Å—å–º–∞', '–ø–∏—Å–µ–º')
                return f"–ù–∞–π–¥–µ–Ω–æ {count} {word}" if count > 0 else "–ü–∏—Å–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
            elif 'file' in item_type or 'document' in item_type or 'doc' in item_type or '—Ñ–∞–π–ª' in tool_lower:
                word = pluralize_ru(count, '—Ñ–∞–π–ª', '—Ñ–∞–π–ª–∞', '—Ñ–∞–π–ª–æ–≤')
                return f"–ù–∞–π–¥–µ–Ω–æ {count} {word}" if count > 0 else "–§–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
            elif 'contact' in item_type or '–∫–æ–Ω—Ç–∞–∫—Ç' in tool_lower:
                word = pluralize_ru(count, '–∫–æ–Ω—Ç–∞–∫—Ç', '–∫–æ–Ω—Ç–∞–∫—Ç–∞', '–∫–æ–Ω—Ç–∞–∫—Ç–æ–≤')
                return f"–ù–∞–π–¥–µ–Ω–æ {count} {word}" if count > 0 else "–ö–æ–Ω—Ç–∞–∫—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
            elif 'task' in item_type or '–∑–∞–¥–∞—á' in tool_lower:
                word = pluralize_ru(count, '–∑–∞–¥–∞—á–∞', '–∑–∞–¥–∞—á–∏', '–∑–∞–¥–∞—á')
                return f"–ù–∞–π–¥–µ–Ω–æ {count} {word}" if count > 0 else "–ó–∞–¥–∞—á –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
            else:
                word = pluralize_ru(count, '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
                return f"–ù–∞–π–¥–µ–Ω–æ {count} {word}" if count > 0 else "–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
        
        # Handle success/error patterns
        if 'success' in result_lower or 'successfully' in result_lower:
            return "‚úì –í—ã–ø–æ–ª–Ω–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ"
        if 'error' in result_lower or 'failed' in result_lower:
            return "‚úó –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"
        if 'created' in result_lower:
            return "‚úì –°–æ–∑–¥–∞–Ω–æ"
        if 'sent' in result_lower:
            return "‚úì –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ"
        if 'updated' in result_lower:
            return "‚úì –û–±–Ω–æ–≤–ª–µ–Ω–æ"
        if 'deleted' in result_lower:
            return "‚úì –£–¥–∞–ª–µ–Ω–æ"
        
        # Default: truncate result
        if len(result) > 50:
            return result[:47] + "..."
        return result if result else "–í—ã–ø–æ–ª–Ω–µ–Ω–æ"

    async def _stream_reasoning(self, event_type: str, data: Dict[str, Any]):
        """Stream reasoning event to WebSocket - Cursor-style intent blocks only."""
        try:
            connection_count = self.ws_manager.get_connection_count(self.session_id)
            if connection_count > 0:
                # Only send intent events (Cursor-style) - no legacy events
                if event_type == "react_thinking":
                    # Don't start intent on thinking - wait for action
                    pass
                
                elif event_type == "react_action":
                    # Start NEW intent with action description
                    tool = data.get("tool", "unknown")
                    action = data.get("action", "")
                    
                    # Save tool for later use in observation
                    self._last_tool = tool
                    
                    # Create human-readable description for the intent
                    description = self._transform_to_human_readable(action, tool)
                    
                    # Start new intent for this action
                    self._current_intent_id = f"intent-{int(time.time() * 1000)}"
                    await self.ws_manager.send_event(
                        self.session_id,
                        "intent_start",
                        {
                            "intent_id": self._current_intent_id,
                            "text": description
                        }
                    )
                
                elif event_type == "react_observation":
                    # Add result details and complete the intent
                    if hasattr(self, '_current_intent_id') and self._current_intent_id:
                        result = str(data.get("result", ""))
                        tool = getattr(self, '_last_tool', 'unknown')
                        
                        # Format result into human-readable Russian summary
                        summary = self._format_result_summary(result, tool)
                        
                        # Extract and send result details (e.g., meeting names, file names)
                        details = self._extract_result_details(result)
                        for detail in details:
                            await self.ws_manager.send_event(
                                self.session_id,
                                "intent_detail",
                                {
                                    "intent_id": self._current_intent_id,
                                    "type": "analyze",
                                    "description": detail
                                }
                            )
                        
                        # Complete intent with summary (for collapsed header)
                        await self.ws_manager.send_event(
                            self.session_id,
                            "intent_complete",
                            {
                                "intent_id": self._current_intent_id,
                                "summary": summary,
                                "auto_collapse": True
                            }
                        )
                        self._current_intent_id = None
                
            else:
                logger.debug(f"[UnifiedReActEngine] Skipping event {event_type} - no WebSocket connection")
        except Exception as e:
            logger.debug(f"[UnifiedReActEngine] Failed to send event {event_type}: {e}")

